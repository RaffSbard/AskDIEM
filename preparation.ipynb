{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *<b>LLAMAINDEX</b>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "os.environ['COHERE_API_KEY'] = os.getenv('COHERE_API_KEY')\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "os.environ[\"QDRANT__API_KEY\"] = os.getenv(\"QDRANT__API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting default options for llama index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "# Settings.embed_model = GoogleGenAIEmbedding(\n",
    "#     model_name=\"gemini-embedding-001\",\n",
    "# )\n",
    "\n",
    "Settings.llm = GoogleGenAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def save_to_pickle(data, filepath):\n",
    "    \"\"\"Salva qualsiasi oggetto Python in un file pickle.\"\"\"\n",
    "    # print(f\"Salvataggio di {len(data)} oggetti in '{filepath}'...\")\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(\"Salvataggio completato.\")\n",
    "\n",
    "def load_from_pickle(filepath):\n",
    "    \"\"\"Carica qualsiasi oggetto Python da un file pickle.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File di cache '{filepath}' non trovato.\")\n",
    "        return []\n",
    "        \n",
    "    print(f\"Caricamento oggetti dalla cache '{filepath}'...\")\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    # print(f\"Caricati {len(data)} oggetti.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- SEZIONE 0: IMPORTAZIONI E CONFIGURAZIONE GLOBALE ---\n",
    "# ==============================================================================\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse, parse_qs, urlencode\n",
    "from collections import deque\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import tqdm\n",
    "import io\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Import per Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Import per LlamaIndex\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "# Import per Qdrant\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from MCER import MainContentExtractorReader\n",
    "\n",
    "# File dove salvare lo stato delle pagine (ETag, Last-Modified, e hash)\n",
    "STATE_FILE = \"data/page_update_state.json\"\n",
    "ALL_URLS_FILE = \"urls_lists/urls_html_master_list.txt\"\n",
    "ALL_URLS_PDF_FILE = \"urls_lists/urls_pdf_master_list.txt\"\n",
    "DOWNLOADED_PDF_URLS_FILE = \"urls_lists/urls_pdf_downloaded_list.txt\"\n",
    "NODES_OUTPUT_FILE = \"nodes/nodes_final_enriched.pkl\"\n",
    "\n",
    "QDRANT_COLLECTION_NAME = \"diem_chatbot3\"\n",
    "\n",
    "# Configurazione per l'estrazione metadati\n",
    "LLM_MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "MIN_DELAY_SECONDS = 1\n",
    "MAX_DELAY_SECONDS = 2\n",
    "\n",
    "# Header da inviare per simulare un browser reale\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEZIONE 1: FUNZIONI DI SUPPORTO ---\n",
    "# ==============================================================================\n",
    "\n",
    "def save_to_pickle(data, filepath):\n",
    "    \"\"\"Salva qualsiasi oggetto Python in un file pickle.\"\"\"\n",
    "    print(f\"Salvataggio di {len(data)} oggetti in '{filepath}'...\")\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(\"Salvataggio completato.\")\n",
    "\n",
    "def load_from_pickle(filepath):\n",
    "    \"\"\"Carica qualsiasi oggetto Python da un file pickle.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File di cache '{filepath}' non trovato.\")\n",
    "        return []\n",
    "        \n",
    "    print(f\"Caricamento oggetti dalla cache '{filepath}'...\")\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    print(f\"Caricati {len(data)} oggetti.\")\n",
    "    return data\n",
    "\n",
    "def load_state(filepath):\n",
    "    \"\"\"Carica in modo sicuro lo stato precedente da un file JSON.\"\"\"\n",
    "    if not os.path.exists(filepath) or os.path.getsize(filepath) == 0:\n",
    "        return {}\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except (json.JSONDecodeError, IOError) as e:\n",
    "        print(f\"Attenzione: impossibile leggere il file di stato '{filepath}'. Parto da uno stato vuoto. Errore: {e}\")\n",
    "        return {}\n",
    "\n",
    "def save_state(filepath, state):\n",
    "    \"\"\"Salva lo stato corrente in un file JSON.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(state, f, indent=4)\n",
    "        print(f\"\\nStato aggiornato salvato con successo in '{filepath}'.\")\n",
    "    except IOError as e:\n",
    "        print(f\"Errore critico: impossibile salvare il file di stato '{filepath}'. Errore: {e}\")\n",
    "\n",
    "def get_content_hash(content):\n",
    "    \"\"\"Calcola l'hash SHA256 del contenuto testuale di una pagina.\"\"\"\n",
    "    return hashlib.sha256(content.encode('utf-8')).hexdigest()\n",
    "\n",
    "def clean_and_validate_url(url):\n",
    "    \"\"\"\n",
    "    Pulisce un URL rimuovendo parametri specifici e il fragment.\n",
    "    Restituisce l'URL pulito e un flag booleano che è True se la struttura\n",
    "    è quella del DIEM (300638) o se non è specificata.\n",
    "    \"\"\"\n",
    "    default_params_to_remove = {'bando', 'progetto', 'lettera', 'avvisi', 'coorte', 'schemaid', 'schemaId', 'adCodFraz', 'adCodRadice', 'annoOfferta', 'annoOrdinamento'}\n",
    "    DIEM_STRUCTURE_ID = '300638'\n",
    "    \n",
    "    # Scomponi l'URL e la sua query in un dizionario\n",
    "    parsed_url = urlparse(url)\n",
    "    query_dict = parse_qs(parsed_url.query)\n",
    "\n",
    "    is_diem_structure = True # Assumiamo che sia valido di default\n",
    "    \n",
    "    params_to_remove_this_time = default_params_to_remove.copy()\n",
    "    # Controlla il parametro 'struttura'\n",
    "    if 'struttura' in query_dict:\n",
    "        # Se mi trovo nella sezione strutture della rubrica allora rimuovo anche \"struttura\"\n",
    "        if \"https://rubrica.unisa.it/strutture\" in url:\n",
    "            params_to_remove_this_time.add('struttura')\n",
    "        # Se il parametro esiste ma il suo valore non è quello corretto\n",
    "        elif query_dict['struttura'][0] != DIEM_STRUCTURE_ID:\n",
    "            is_diem_structure = False\n",
    "            \n",
    "    # Controlla il parametro 'cdsStruttura'\n",
    "    elif 'cdsStruttura' in query_dict:\n",
    "        # Se il parametro esiste ma il suo valore non è quello corretto\n",
    "        if query_dict['cdsStruttura'][0] != DIEM_STRUCTURE_ID:\n",
    "            is_diem_structure = False\n",
    "\n",
    "    # Controlla l'url base per casi speciali\n",
    "    elif 'https://www.diem.unisa.it/home/bandi' in url and 'modulo' in query_dict and query_dict['modulo'][0] != '226':\n",
    "        is_diem_structure = False\n",
    "\n",
    "    # Aggiungi il parametro 'anno' se non esiste\n",
    "    if 'https://www.diem.unisa.it/home/bandi' in url and 'modulo' in query_dict and 'anno' not in query_dict:\n",
    "        query_dict['anno'] = ['2025']  \n",
    "\n",
    "    # Logica di pulizia dei parametri\n",
    "    if 'bando' in query_dict and 'idConcorso' in query_dict:\n",
    "        params_to_remove_this_time.remove('bando')\n",
    "    \n",
    "    # Rimuovi le chiavi indesiderate\n",
    "    for param in params_to_remove_this_time:\n",
    "        query_dict.pop(param, None)\n",
    "        \n",
    "    # Ricostruisci la stringa di query e l'URL\n",
    "    new_query_string = urlencode(query_dict, doseq=True)\n",
    "    clean_parsed_url = parsed_url._replace(query=new_query_string, fragment=\"\")\n",
    "    cleaned_url = clean_parsed_url.geturl()\n",
    "    \n",
    "    # Restituisce sia l'URL pulito che il flag\n",
    "    return cleaned_url, is_diem_structure\n",
    "\n",
    "def get_insegnamento_signature(url):\n",
    "    \"\"\"\n",
    "    Se l'URL è di tipo 'insegnamenti', calcola la sua \"firma\" unica\n",
    "    rimuovendo il penultimo segmento del percorso. Altrimenti, restituisce None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path_segments = urlparse(url).path.strip('/').split('/')\n",
    "        if len(path_segments) < 2:\n",
    "            return None\n",
    "        signature = tuple(path_segments[:-2] + path_segments[-1:])\n",
    "        return signature\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def make_markdown_links_absolute(markdown_text, base_url):\n",
    "    def replacer(match):\n",
    "        link_text = match.group(1)\n",
    "        link_url = match.group(2)\n",
    "        absolute_url = urljoin(base_url, link_url)\n",
    "        return f\"[{link_text}]({absolute_url})\"\n",
    "    markdown_link_pattern = r'\\[([^\\]]+)\\]\\(([^)]+)\\)'\n",
    "    return re.sub(markdown_link_pattern, replacer, markdown_text)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEZIONE 2: CONTROLLO DEGLI AGGIORNAMENTI ---\n",
    "# ==============================================================================\n",
    "\n",
    "def check_for_updates_robust(urls_to_check, last_state):\n",
    "    \"\"\"\n",
    "    Controlla una lista di URL usando una strategia ibrida basata solo su richieste GET.\n",
    "    \"\"\"\n",
    "    updated_urls = []\n",
    "    current_state = {}\n",
    "    \n",
    "    print(f\"Controllo di {len(urls_to_check)} URL per aggiornamenti...\")\n",
    "    \n",
    "    for url in urls_to_check:\n",
    "        print(f\"\\n-> Controllando: {url}\")\n",
    "        previous_data = last_state.get(url, {})\n",
    "        request_headers = HEADERS.copy()\n",
    "\n",
    "        # Aggiungi gli header di caching se li abbiamo salvati\n",
    "        if previous_data.get(\"etag\"):\n",
    "            request_headers[\"If-None-Match\"] = previous_data[\"etag\"]\n",
    "        if previous_data.get(\"last_modified\"):\n",
    "            request_headers[\"If-Modified-Since\"] = previous_data[\"last_modified\"]\n",
    "\n",
    "        try:\n",
    "            with requests.get(url, headers=request_headers, timeout=10, allow_redirects=True, stream=True) as response:\n",
    "                time.sleep(0.5)\n",
    "\n",
    "                # 1. CONTROLLO EFFICIENTE TRAMITE HEADER\n",
    "                if response.status_code == 304: # 304 Not Modified\n",
    "                    print(\"   Stato: Non modificato (304 via GET).\")\n",
    "                    current_state[url] = previous_data\n",
    "                    continue\n",
    "                \n",
    "                # Se il server risponde 200 OK e fornisce header di caching, li usiamo\n",
    "                # senza scaricare l'intero contenuto.\n",
    "                if response.status_code == 200 and (response.headers.get(\"ETag\") or response.headers.get(\"Last-Modified\")):\n",
    "                    print(\"   Stato: Aggiornato (rilevato via header GET). Salvo nuovi ETag/Last-Modified.\")\n",
    "                    updated_urls.append(url)\n",
    "                    current_state[url] = {\n",
    "                        \"etag\": response.headers.get(\"ETag\"),\n",
    "                        \"last_modified\": response.headers.get(\"Last-Modified\"),\n",
    "                        \"content_hash\": None # Resettiamo l'hash\n",
    "                    }\n",
    "                    continue\n",
    "\n",
    "                # 2. FALLBACK SU HASH DEL CONTENUTO\n",
    "                # Solo se il server risponde 200 OK ma non fornisce header di caching,\n",
    "                # procediamo a scaricare l'intero contenuto.\n",
    "                if response.status_code == 200:\n",
    "                    print(\"   Info: Il server non supporta caching efficiente. Eseguo fallback su hash del contenuto.\")\n",
    "                    \n",
    "                    # Scarica il contenuto del body\n",
    "                    content = response.text\n",
    "                    new_hash = get_content_hash(content)\n",
    "                    old_hash = previous_data.get(\"content_hash\")\n",
    "                    \n",
    "                    if new_hash != old_hash:\n",
    "                        print(f\"   Stato: Aggiornato (rilevato via hash). Hash: {new_hash[:10]}... (precedente: {str(old_hash)[:10]}...)\")\n",
    "                        updated_urls.append(url)\n",
    "                        current_state[url] = {\n",
    "                            \"etag\": None,\n",
    "                            \"last_modified\": None,\n",
    "                            \"content_hash\": new_hash\n",
    "                        }\n",
    "                    else:\n",
    "                        print(\"   Stato: Non modificato (hash identico).\")\n",
    "                        current_state[url] = previous_data\n",
    "                else:\n",
    "                    # Gestisce altri status code (es. 403, 404, 500)\n",
    "                    response.raise_for_status()\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"   ERRORE: Impossibile controllare l'URL. Errore: {e}\")\n",
    "            if url in last_state:\n",
    "                current_state[url] = last_state[url]\n",
    "            \n",
    "    return updated_urls, current_state\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEZIONE 3: CRAWLER ---\n",
    "# ==============================================================================\n",
    "\n",
    "def run_crawler(start_urls, pre_visited_urls):\n",
    "    \"\"\"\n",
    "    Esegue il crawler partendo da una lista di URL fornita.\n",
    "    \"\"\"\n",
    "    if not start_urls:\n",
    "        print(\"\\n--- FASE 2: Nessuna pagina aggiornata da cui partire. Crawling non avviato. ---\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- FASE 2: Inizio crawling da {len(start_urls)} pagine aggiornate ---\")\n",
    "\n",
    "    # Imposta e avvia il browser automatizzato\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless') # Esegue Chrome in background, senza aprire una finestra\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    # --- IMPOSTAZIONI ---\n",
    "    ALLOWED_DOMAINS = [\"www.diem.unisa.it\", \"rubrica.unisa.it\", \"docenti.unisa.it\", \"easycourse.unisa.it\", \"web.unisa.it\", \"corsi.unisa.it\", \"unisa.coursecatalogue.cineca.it\"]\n",
    "    DOMAINS_REQUIRING_JS = {\"unisa.coursecatalogue.cineca.it\"}\n",
    "    EXCLUDED_LANGUAGES = {'en', 'es', 'de', 'fr', 'zh'}\n",
    "\n",
    "    # --- STRUTTURE DATI ---\n",
    "    # La coda mantiene la logica del percorso per la stampa a schermo\n",
    "    urls_to_visit = deque([(url, [url]) for url in start_urls])\n",
    "    visited_urls = pre_visited_urls # Inizializza con gli URL già visitati\n",
    "    seen_insegnamenti_signatures = set() # Per evitare duplicati in \"insegnamenti\"\n",
    "    newly_found_urls = set()\n",
    "\n",
    "    while urls_to_visit:\n",
    "        current_url, current_path = urls_to_visit.popleft()\n",
    "\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "\n",
    "        path_str = \" -> \".join(current_path)\n",
    "        print(f\"-> Visitando: {path_str}\")\n",
    "        \n",
    "        visited_urls.add(current_url)\n",
    "        page_content = None\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            current_domain = urlparse(current_url).netloc\n",
    "\n",
    "            if current_domain in DOMAINS_REQUIRING_JS:\n",
    "                try:\n",
    "                    # Se il dominio richiede JS, usiamo Selenium\n",
    "                    driver.get(current_url)\n",
    "                    wait = WebDriverWait(driver, 10) # Aspetta massimo 10 secondi\n",
    "                    wait.until(\n",
    "                        EC.visibility_of_element_located((By.CSS_SELECTOR, \"main.app-main-container\"))\n",
    "                    )\n",
    "                    page_content = driver.page_source\n",
    "                except TimeoutException:\n",
    "                    print(f\"Timeout durante l'attesa del contenuto dinamico per {current_url}\")\n",
    "                    page_content = None # Se non carica, non abbiamo contenuto da analizzare\n",
    "                except Exception as e:\n",
    "                    print(f\"Un altro errore di Selenium è occorso per {current_url}: {e}\")\n",
    "                    page_content = None\n",
    "            else:\n",
    "                # Altrimenti, usiamo 'requests'\n",
    "                response = requests.get(current_url, timeout=10)\n",
    "                if response.status_code == 200 and 'text/html' in response.headers.get('Content-Type', ''):\n",
    "                    page_content = response.content\n",
    "                \n",
    "            if page_content:\n",
    "                newly_found_urls.add(current_url)\n",
    "\n",
    "                soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "                # Gestione dei link relativi senza slash iniziale\n",
    "                parsed_current_url = urlparse(current_url)\n",
    "                base_for_join = current_url\n",
    "                if not current_url.endswith('/') and '.' not in parsed_current_url.path.split('/')[-1]:\n",
    "                    base_for_join += '/'\n",
    "\n",
    "                for link in soup.find_all('a', href=True):\n",
    "\n",
    "                    href = link['href']\n",
    "                    absolute_url = urljoin(base_for_join, href)\n",
    "                    parsed_url = urlparse(absolute_url)  \n",
    "                    path_segments = parsed_url.path.split('/')\n",
    "                    clean_url = parsed_url._replace(fragment=\"\").geturl()\n",
    "                    param_cleaned_url, is_diem_structure = clean_and_validate_url(clean_url)\n",
    "                    new_domain = parsed_url.netloc\n",
    "                    path = urlparse(clean_url).path\n",
    "                    module_count = path.count(\"/module/\")\n",
    "                    row_count = path.count(\"/row/\")\n",
    "\n",
    "                    if new_domain not in ALLOWED_DOMAINS or EXCLUDED_LANGUAGES.intersection(path_segments) or \"sitemap\" in clean_url or \\\n",
    "                    (\"unisa-rescue-page\" in clean_url and ((module_count > 1 and row_count > 1) or \"/uploads/rescue/\" in clean_url)) or not is_diem_structure or \\\n",
    "                        clean_url.endswith(('.pdf', '.doc', '.docx', '.jpg', '.png', '.htm')) or clean_url.startswith(\"http://\"): \n",
    "                        continue\n",
    "\n",
    "                    should_add = False\n",
    "                    if new_domain == \"www.diem.unisa.it\" and current_domain == \"www.diem.unisa.it\":\n",
    "                        should_add = True\n",
    "                    elif new_domain == \"rubrica.unisa.it\" and current_domain == \"www.diem.unisa.it\":\n",
    "                        should_add = True\n",
    "                    elif new_domain == \"docenti.unisa.it\" and (current_domain == \"rubrica.unisa.it\" or (current_domain == \"docenti.unisa.it\" and ((\"curriculum\" in clean_url and not clean_url.endswith(\"/\")) or (\"didattica\" in clean_url and \"didattica\" not in current_url)))) and clean_url != \"https://docenti.unisa.it\" and \"simona.mancini\" not in clean_url:\n",
    "                        should_add = True\n",
    "                    elif new_domain == \"easycourse.unisa.it\" and (\"Dipartimento_di_Ingegneria_dellInformazione_ed_Elettrica_e_Matematica_Applicata\" in clean_url or \"Facolta_di_Ingegneria_-_Esami\" in clean_url):\n",
    "                        if (\"index\" in current_url and (\"ttCdlHtml\" not in clean_url and \"index\" not in clean_url)) or \"ttCdlHtml\" in current_url: \n",
    "                            should_add = False\n",
    "                        else:\n",
    "                            should_add = True\n",
    "                    elif new_domain == \"web.unisa.it\" and (current_domain == \"www.diem.unisa.it\" or current_domain == \"web.unisa.it\") and \"servizi-on-line\" in clean_url:\n",
    "                        should_add = True\n",
    "                    elif new_domain == \"corsi.unisa.it\" and (current_domain == \"www.diem.unisa.it\" or current_domain == \"corsi.unisa.it\") and clean_url != \"https://corsi.unisa.it\" and clean_url != \"http://corsi.unisa.it\" and \"unisa-rescue-page\" not in clean_url and \"news\" not in clean_url and \"occupazione-spazi\" not in clean_url and not re.search(r\"^https://corsi\\.unisa\\.it/\\d{5,}\", clean_url):\n",
    "                        should_add = True\n",
    "                    elif new_domain == \"unisa.coursecatalogue.cineca.it\" and (current_domain == \"corsi.unisa.it\" or current_domain == \"unisa.coursecatalogue.cineca.it\") and clean_url != \"https://unisa.coursecatalogue.cineca.it/\" and \"gruppo\" not in clean_url and \"cerca-\" not in clean_url and \"support.apple.com\" not in clean_url and \"WWW.ESSE3WEB.UNISA.IT\" not in clean_url:\n",
    "                        signature = get_insegnamento_signature(param_cleaned_url)\n",
    "                        if not signature or signature in seen_insegnamenti_signatures:\n",
    "                            continue # Salta se la firma non è valida o è già stata vista\n",
    "                        \n",
    "                        seen_insegnamenti_signatures.add(signature)\n",
    "                        should_add = True\n",
    "                    \n",
    "                    if should_add:\n",
    "                        if param_cleaned_url not in visited_urls:\n",
    "                            new_path = current_path + [param_cleaned_url]\n",
    "                            urls_to_visit.append((param_cleaned_url, new_path))\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Errore durante la richiesta a {current_url}: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    print(\"\\nCrawling completato.\")\n",
    "    return newly_found_urls\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEZIONE 4: LOGICA DI ELABORAZIONE DEL CONTENUTO ---\n",
    "# ==============================================================================\n",
    "\n",
    "def process_urls_to_documents(urls_to_process):\n",
    "    \"\"\"\n",
    "    Prende una lista di URL, estrae il contenuto principale, lo elabora\n",
    "    e salva il risultato in un file pickle.\n",
    "    \"\"\"\n",
    "    if not urls_to_process:\n",
    "        print(\"\\nFASE 3: Nessun nuovo documento da elaborare.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFASE 3: Elaborazione del contenuto di {len(urls_to_process)} pagine...\")\n",
    "\n",
    "    # 1. Estrazione del blocco HTML principale\n",
    "    loader = MainContentExtractorReader()\n",
    "    html_documents = loader.load_data(urls=urls_to_process)\n",
    "\n",
    "    # 2. Aggiunta dei metadati\n",
    "    for doc, url in zip(html_documents, urls_to_process):\n",
    "        doc.metadata[\"source_url\"] = url\n",
    "    print(f\"Metadato 'source_url' aggiunto a {len(html_documents)} documenti.\")\n",
    "\n",
    "    # 3. Elaborazione del testo e creazione dei documenti finali\n",
    "    processed_documents = []\n",
    "    for doc in html_documents:\n",
    "        base_url = doc.metadata.get(\"source_url\", \"\")\n",
    "        clean_text_with_links = make_markdown_links_absolute(doc.text, base_url)\n",
    "        processed_documents.append(\n",
    "            Document(text=clean_text_with_links, metadata=doc.metadata)\n",
    "        )\n",
    "\n",
    "    # 4. Salvataggio del risultato\n",
    "    print(f\"Elaborati {len(processed_documents)} documenti.\")\n",
    "    return processed_documents\n",
    "\n",
    "def process_pdfs(url_list_file, url_list_downloaded_file):\n",
    "    \"\"\"\n",
    "    Legge gli URL dei PDF, li scarica in memoria, estrae il testo \n",
    "    e restituisce una lista di Document di LlamaIndex.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(url_list_file, \"r\") as f:\n",
    "        urls_to_process = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"Trovati {len(urls_to_process)} PDF da processare in memoria.\")\n",
    "\n",
    "    with open(url_list_downloaded_file, \"r\") as f:\n",
    "        already_downloaded_urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    pdfs_to_process = [url for url in urls_to_process if url not in already_downloaded_urls]\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    pdf_documents = []\n",
    "\n",
    "    for url in pdfs_to_process:\n",
    "        try:\n",
    "            print(f\"-> Processando in memoria: {url}\")\n",
    "            response = requests.get(url, timeout=30, headers=headers)\n",
    "            response.raise_for_status() # Controlla errori HTTP\n",
    "\n",
    "            # Assicurati che sia un PDF prima di continuare\n",
    "            if 'application/pdf' not in response.headers.get('content-type', ''):\n",
    "                print(f\"  [SKIPPATO] L'URL non è un PDF, ma: {response.headers.get('content-type')}\")\n",
    "                continue\n",
    "\n",
    "            pdf_bytes = io.BytesIO(response.content)\n",
    "            \n",
    "            reader = PdfReader(pdf_bytes)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() or \"\" # Aggiungi \"\" se la pagina è vuota\n",
    "            \n",
    "            if not text.strip():\n",
    "                print(\"  [SKIPPATO] Il PDF è vuoto o contiene solo immagini (no testo).\")\n",
    "                continue\n",
    "\n",
    "            doc = Document(\n",
    "                text=text,\n",
    "                metadata={\n",
    "                    \"source_url\": url,\n",
    "                }\n",
    "            )\n",
    "            pdf_documents.append(doc)\n",
    "\n",
    "            # Aggiungi l'URL alla lista dei già scaricati\n",
    "            with open(url_list_downloaded_file, \"a\") as f:\n",
    "                f.write(url + \"\\n\")\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"  [ERRORE DOWNLOAD]: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERRORE LETTURA PDF]: {e}\")\n",
    "\n",
    "    return pdf_documents\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEZIONE 5: ARRICCHIMENTO METADATI E CREAZIONE NODI ---\n",
    "# ==============================================================================\n",
    "\n",
    "def enrich_documents_with_metadata(documents):\n",
    "    \"\"\"\n",
    "    Arricchisce una lista di documenti con metadati generati da un LLM.\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        print(\"\\nFASE 4: Nessun documento da arricchire con metadati.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\nFASE 4: Inizio arricchimento metadati per {len(documents)} documenti...\")\n",
    "    gemini_model = GoogleGenAI(model=LLM_MODEL_NAME, temperature=0.2)\n",
    "\n",
    "    for document in tqdm.tqdm(documents, desc=\"Arricchendo documenti\"):\n",
    "        if not isinstance(document.text, str) or not document.text.strip():\n",
    "            continue\n",
    "        \n",
    "        # Estratto dal tuo codice di estrazione\n",
    "        prompt = (\n",
    "            \"Analizza il seguente documento, inclusa la sua URL di origine, per estrarre i metadati richiesti. \"\n",
    "            \"Fornisci l'output esclusivamente in formato JSON, seguendo la struttura e le regole specificate.\\n\\n\"\n",
    "            \"--- DOCUMENTO ---\\n\"\n",
    "            f'\"\"\"{document.text}\"\"\"\\n'\n",
    "            \"--- FINE DOCUMENTO ---\\n\\n\"\n",
    "            \n",
    "            \"REGOLE GENERALI:\\n\"\n",
    "            \"- Se il documento è troppo breve o non contiene informazioni sufficienti per generare un campo specifico (title, summary, etc.), lascia quel campo vuoto (es. `\\\"title\\\": \\\"\\\"` o `\\\"questions\\\": []`).\\n\"\n",
    "            \"- **Keywords**: Estrai un numero di parole chiave proporzionale alla lunghezza del testo, fino a un massimo di 10. Per documenti molto brevi, poche parole chiave (o nessuna) sono accettabili.\\n\"\n",
    "            \"- **Domande**: Genera un numero di domande proporzionale alla lunghezza del testo, fino a un massimo di 3. Per documenti molto brevi, una sola domanda o nessuna sono accettabili.\\n\\n\"\n",
    "\n",
    "            \"REGOLE PER 'years':\\n\"\n",
    "            \"- Identifica l'anno o gli anni accademici (es. '2024/2025') o solari (es. '2023') *PRINCIPALI* del documento. Controlla sia il testo che l'URL di origine.\\n\"\n",
    "            \"- Se il documento tratta un singolo anno, inserisci solo quello. Esempio: [\\\"2023\\\"].\\n\"\n",
    "            \"- Se nell'URL è presente il parametro 'anno', considera il suo valore corrispondente come UNICO anno principale, NON considerare il parametro quando 'anno=0'.\\n\"\n",
    "            \"- Se tratta più anni, IDENTIFICA QUELLO PRINCIPALE ed inseriscilo, altrimenti inseriscili tutti. Esempio: [\\\"2022\\\", \\\"2023\\\", \\\"2024\\\"].\\n\"\n",
    "            \"- Se non riesci ad identificare l'anno principale dal testo, ma è presente nell'URL, usa quello. Esempio: [\\\"2023\\\"] se l'URL contiene 'anno=2023' (sempre escludendo il caso in cui sia 'anno=0').\\n\"\n",
    "            \"- Se non ha un anno di riferimento, lascia la lista vuota. Esempio: [].\\n\\n\"\n",
    "\n",
    "            \"Formato JSON richiesto:\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \"title\": \"Un titolo conciso e descrittivo del documento\",\\n'\n",
    "            '  \"summary\": \"Un riassunto di 2-3 frasi del contenuto principale\",\\n'\n",
    "            '  \"questions\": [],\\n'  # Da 0 a 3 domande in base alla lunghezza del testo\n",
    "            '  \"keywords\": [],\\n'   # Da 0 a 10 parole chiave in base alla lunghezza del testo\n",
    "            '  \"years\": []\\n'\n",
    "            \"}\\n\\n\"\n",
    "            \"Output JSON:\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = gemini_model.complete(prompt)\n",
    "            cleaned_response = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            metadata = json.loads(cleaned_response)\n",
    "            document.metadata.update(metadata) # Aggiorna direttamente i metadati del documento\n",
    "        except Exception as e:\n",
    "            print(f\"\\nErrore durante l'arricchimento del documento {document.metadata.get('source_url', 'N/A')}: {e}\")\n",
    "        \n",
    "        time.sleep(random.uniform(MIN_DELAY_SECONDS, MAX_DELAY_SECONDS))\n",
    "        \n",
    "    print(\"Arricchimento metadati completato.\")\n",
    "    return documents\n",
    "\n",
    "def create_nodes_from_documents(documents, output_filepath):\n",
    "    \"\"\"\n",
    "    Prende una lista di documenti arricchiti e li trasforma in nodi.\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        print(\"\\nFASE 5: Nessun documento da trasformare in nodi.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFASE 5: Creazione dei nodi da {len(documents)} documenti...\")\n",
    "    \n",
    "    sentence_splitter = SentenceSplitter(chunk_size=512*16, chunk_overlap=512*2)\n",
    "    pipeline = IngestionPipeline(transformations=[sentence_splitter])\n",
    "    \n",
    "    nodes = pipeline.run(documents=documents, show_progress=True)\n",
    "    \n",
    "    save_to_pickle(nodes, output_filepath)\n",
    "    print(f\"Creati e salvati {len(nodes)} nodi in '{output_filepath}'.\")\n",
    "\n",
    "    return nodes\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEZIONE 6: INDICIZZAZIONE SU QDRANT ---\n",
    "# ==============================================================================\n",
    "\n",
    "def index_nodes_to_qdrant(nodes_to_index):\n",
    "    \"\"\"\n",
    "    Indicizza una lista di nodi in una collezione Qdrant.\n",
    "    Crea la collezione se non esiste, altrimenti aggiunge i nodi.\n",
    "    \"\"\"\n",
    "    if not nodes_to_index:\n",
    "        print(\"\\nFASE 6: Nessun nuovo nodo da indicizzare.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFASE 6: Inizio indicizzazione di {len(nodes_to_index)} nodi su Qdrant...\")\n",
    "\n",
    "    # 1. Connettiti a Qdrant\n",
    "    client = QdrantClient(url=\"https://e542824d-6590-4005-91db-6dd34bf8f471.eu-west-2-0.aws.cloud.qdrant.io:6333\", api_key=os.getenv(\"QDRANT__API_KEY\"))\n",
    "    vector_store = QdrantVectorStore(client=client, collection_name=QDRANT_COLLECTION_NAME)\n",
    "\n",
    "    # 2. Controlla se la collezione esiste già\n",
    "    try:\n",
    "        # Questo comando fallisce se la collezione non esiste\n",
    "        client.get_collection(collection_name=QDRANT_COLLECTION_NAME)\n",
    "        collection_exists = True\n",
    "        print(f\"La collezione '{QDRANT_COLLECTION_NAME}' esiste già. Aggiungo i nuovi nodi.\")\n",
    "    except Exception:\n",
    "        collection_exists = False\n",
    "        print(f\"La collezione '{QDRANT_COLLECTION_NAME}' non esiste. Verrà creata.\")\n",
    "\n",
    "    # 3. Indicizza i nodi\n",
    "    if collection_exists:\n",
    "        # Carica l'indice esistente e inserisci i nuovi nodi\n",
    "        index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "        index.insert_nodes(nodes_to_index, show_progress=True)\n",
    "    else:\n",
    "        # Crea l'indice da zero con i nuovi nodi\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        index = VectorStoreIndex(nodes_to_index, storage_context=storage_context, show_progress=True)\n",
    "    \n",
    "    print(\"Indicizzazione su Qdrant completata con successo.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- SEZIONE 7: ESECUZIONE DEL FLUSSO INTEGRATO ---\n",
    "# ==============================================================================\n",
    "\n",
    "def main_workflow():\n",
    "    \"\"\" Esegue il flusso completo di controllo aggiornamenti e crawling. \"\"\"\n",
    "    \n",
    "    # 1. Carica la lista master di URL da monitorare\n",
    "    if os.path.exists(ALL_URLS_FILE):\n",
    "        with open(ALL_URLS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            urls_to_monitor = [line.strip() for line in f if line.strip()]\n",
    "    else:\n",
    "        # Se non hai una lista master, il crawler partirà da un punto fisso\n",
    "        # e il controllo aggiornamenti verrà saltato la prima volta.\n",
    "        urls_to_monitor = [\"https://www.diem.unisa.it/home?sitemap\"]\n",
    "    \n",
    "    # 2. Controlla gli aggiornamenti\n",
    "    last_known_state = load_state(STATE_FILE)\n",
    "    updated_pages, new_state = check_for_updates_robust(urls_to_monitor, last_known_state)\n",
    "    \n",
    "    # 3. Usa le pagine aggiornate come punto di partenza per il crawler\n",
    "    # Se non ci sono pagine aggiornate, il crawler non parte.\n",
    "    # Se è la prima esecuzione (last_known_state è vuoto), tutte le pagine sono \"nuove\".\n",
    "    start_points_for_crawler = updated_pages\n",
    "    if not last_known_state:\n",
    "        print(\"\\nPrima esecuzione: tutte le pagine verranno considerate 'da visitare'.\")\n",
    "        start_points_for_crawler = urls_to_monitor\n",
    "        # Alla prima esecuzione, non ci sono pagine da ignorare\n",
    "        unchanged_urls = set()\n",
    "    else:\n",
    "        unchanged_urls = set(urls_to_monitor) - set(updated_pages)\n",
    "\n",
    "    crawled_urls = run_crawler(start_points_for_crawler, unchanged_urls)\n",
    "\n",
    "    # 4. Combina le liste e processa il contenuto\n",
    "    # Processiamo sia le pagine che abbiamo rilevato come aggiornate,\n",
    "    # sia quelle nuove che il crawler ha appena scoperto.\n",
    "    # urls_to_process = list(set(updated_pages).union(crawled_urls))\n",
    "\n",
    "    if not crawled_urls:\n",
    "        print(\"\\nNessuna pagina nuova o aggiornata da processare. Flusso di lavoro terminato.\")\n",
    "        save_state(STATE_FILE, new_state) # Salva comunque lo stato\n",
    "        return\n",
    "    \n",
    "    crawled_urls = [url for url in crawled_urls if \"rubrica.unisa.it\" not in url] # Escludi rubrica.unisa.it\n",
    "    \n",
    "    # 5. Estrai il contenuto\n",
    "    newly_processed_htmls = process_urls_to_documents(crawled_urls)\n",
    "    newly_processed_pdf_documents = process_pdfs(ALL_URLS_PDF_FILE, DOWNLOADED_PDF_URLS_FILE)\n",
    "\n",
    "    newly_processed_documents = []\n",
    "    if newly_processed_htmls:\n",
    "        newly_processed_documents.extend(newly_processed_htmls)\n",
    "    if newly_processed_pdf_documents:\n",
    "        newly_processed_documents.extend(newly_processed_pdf_documents)\n",
    "\n",
    "    # 6. Arricchisci con metadati\n",
    "    enriched_documents = enrich_documents_with_metadata(newly_processed_documents)\n",
    "    \n",
    "    # 7. Crea e salva i nodi\n",
    "    new_nodes = create_nodes_from_documents(enriched_documents, NODES_OUTPUT_FILE)\n",
    "\n",
    "    # 8: Indicizza i nodi su Qdrant\n",
    "    index_nodes_to_qdrant(new_nodes)\n",
    "    \n",
    "    # 9. Aggiorna la lista master e lo stato\n",
    "    if crawled_urls:\n",
    "        print(f\"\\nAggiornamento della lista URL master con {len(crawled_urls)} nuove pagine.\")\n",
    "        # Unisci i vecchi URL con i nuovi trovati, rimuovendo duplicati\n",
    "        final_url_set = set(urls_to_monitor).union(crawled_urls)\n",
    "        with open(ALL_URLS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            for url in sorted(list(final_url_set)):\n",
    "                f.write(f\"{url}\\n\")\n",
    "    \n",
    "    # 10. Salva lo stato aggiornato delle pagine per il prossimo controllo\n",
    "    save_state(STATE_FILE, new_state)\n",
    "\n",
    "    print(\"\\nFlusso di lavoro completato.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esegui il flusso completo\n",
    "main_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Imposta e avvia il browser automatizzato\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless') # Esegue Chrome in background, senza aprire una finestra\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# --- IMPOSTAZIONI ---\n",
    "START_URL = [\"https://www.diem.unisa.it/home?sitemap\"]\n",
    "ALLOWED_DOMAINS = [\"www.diem.unisa.it\", \"rubrica.unisa.it\", \"docenti.unisa.it\", \"easycourse.unisa.it\", \"web.unisa.it\", \"corsi.unisa.it\", \"unisa.coursecatalogue.cineca.it\"]\n",
    "DOMAINS_REQUIRING_JS = {\"unisa.coursecatalogue.cineca.it\"}\n",
    "EXCLUDED_LANGUAGES = {'en', 'es', 'de', 'fr', 'zh'}\n",
    "\n",
    "# --- STRUTTURE DATI ---\n",
    "# La coda mantiene la logica del percorso per la stampa a schermo\n",
    "urls_to_visit = deque([(url, [url]) for url in START_URL])\n",
    "seen_insegnamenti_signatures = set() # Per evitare duplicati in \"insegnamenti\"\n",
    "\n",
    "visited_urls = set()\n",
    "filename = \"urls_lists/urls_html_master_list.txt\"\n",
    "try:\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        visited_urls = {line.strip() for line in f if line.strip()}\n",
    "    print(f\"Caricati {len(visited_urls)} URL già visitati dal file '{filename}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{filename}' non trovato. Si parte con un set di URL visitati vuoto.\")\n",
    "\n",
    "html_urls = set()\n",
    "\n",
    "print(f\"Inizio crawling da: {START_URL}\")\n",
    "\n",
    "while urls_to_visit:\n",
    "    current_url, current_path = urls_to_visit.popleft()\n",
    "\n",
    "    if current_url in visited_urls:\n",
    "        continue\n",
    "\n",
    "    path_str = \" -> \".join(current_path)\n",
    "    print(f\"-> Visitando: {path_str}\")\n",
    "    \n",
    "    visited_urls.add(current_url)\n",
    "    page_content = None\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        current_domain = urlparse(current_url).netloc\n",
    "\n",
    "        if current_domain in DOMAINS_REQUIRING_JS:\n",
    "            try:\n",
    "                # Se il dominio richiede JS, usiamo Selenium\n",
    "                driver.get(current_url)\n",
    "                wait = WebDriverWait(driver, 10) # Aumentato a 10 secondi per sicurezza\n",
    "                wait.until(\n",
    "                    EC.visibility_of_element_located((By.CSS_SELECTOR, \"main.app-main-container\"))\n",
    "                )\n",
    "                #time.sleep(3) # Attesa fissa per sicurezza\n",
    "                page_content = driver.page_source\n",
    "            except TimeoutException:\n",
    "                print(f\"Timeout durante l'attesa del contenuto dinamico per {current_url}\")\n",
    "                page_content = None # Se non carica, non abbiamo contenuto da analizzare\n",
    "            except Exception as e:\n",
    "                print(f\"Un altro errore di Selenium è occorso per {current_url}: {e}\")\n",
    "                page_content = None\n",
    "        else:\n",
    "            # Altrimenti, usiamo il veloce 'requests'\n",
    "            response = requests.get(current_url, timeout=10)\n",
    "            if response.status_code == 200 and 'text/html' in response.headers.get('Content-Type', ''):\n",
    "                page_content = response.content\n",
    "            \n",
    "        if page_content:\n",
    "            html_urls.add(current_url)\n",
    "\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "            # Gestione dei link relativi senza slash iniziale\n",
    "            parsed_current_url = urlparse(current_url)\n",
    "            base_for_join = current_url\n",
    "            if not current_url.endswith('/') and '.' not in parsed_current_url.path.split('/')[-1]:\n",
    "                base_for_join += '/'\n",
    "\n",
    "            for link in soup.find_all('a', href=True):\n",
    "\n",
    "                href = link['href']\n",
    "                absolute_url = urljoin(base_for_join, href)\n",
    "                parsed_url = urlparse(absolute_url)  \n",
    "                path_segments = parsed_url.path.split('/')\n",
    "                clean_url = parsed_url._replace(fragment=\"\").geturl()\n",
    "                param_cleaned_url, is_diem_structure = clean_and_validate_url(clean_url)\n",
    "                new_domain = parsed_url.netloc\n",
    "                path = urlparse(clean_url).path\n",
    "                module_count = path.count(\"/module/\")\n",
    "                row_count = path.count(\"/row/\")\n",
    "\n",
    "                if new_domain not in ALLOWED_DOMAINS or EXCLUDED_LANGUAGES.intersection(path_segments) or \"sitemap\" in clean_url or \\\n",
    "                   (\"unisa-rescue-page\" in clean_url and ((module_count > 1 and row_count > 1) or \"/uploads/rescue/\" in clean_url)) or not is_diem_structure or \\\n",
    "                    clean_url.endswith(('.pdf', '.doc', '.docx', '.jpg', '.png', '.htm')) or clean_url.startswith(\"http://\"):\n",
    "                    continue\n",
    "\n",
    "                should_add = False\n",
    "                if new_domain == \"www.diem.unisa.it\" and current_domain == \"www.diem.unisa.it\":\n",
    "                    should_add = True\n",
    "                elif new_domain == \"rubrica.unisa.it\" and current_domain == \"www.diem.unisa.it\":\n",
    "                    should_add = True\n",
    "                elif new_domain == \"docenti.unisa.it\" and (current_domain == \"rubrica.unisa.it\" or (current_domain == \"docenti.unisa.it\" and ((\"curriculum\" in clean_url and not clean_url.endswith(\"/\")) or (\"didattica\" in clean_url and \"didattica\" not in current_url)))) and clean_url != \"https://docenti.unisa.it\" and \"simona.mancini\" not in clean_url:\n",
    "                    should_add = True\n",
    "                elif new_domain == \"easycourse.unisa.it\" and (\"Dipartimento_di_Ingegneria_dellInformazione_ed_Elettrica_e_Matematica_Applicata\" in clean_url or \"Facolta_di_Ingegneria_-_Esami\" in clean_url):\n",
    "                    if (\"index\" in current_url and (\"ttCdlHtml\" not in clean_url and \"index\" not in clean_url)) or \"ttCdlHtml\" in current_url:\n",
    "                        should_add = False\n",
    "                    else:\n",
    "                        should_add = True\n",
    "                elif new_domain == \"web.unisa.it\" and (current_domain == \"www.diem.unisa.it\" or current_domain == \"web.unisa.it\") and \"servizi-on-line\" in clean_url:\n",
    "                    should_add = True\n",
    "                elif new_domain == \"corsi.unisa.it\" and (current_domain == \"www.diem.unisa.it\" or current_domain == \"corsi.unisa.it\") and clean_url != \"https://corsi.unisa.it\" and clean_url != \"http://corsi.unisa.it\" and \"unisa-rescue-page\" not in clean_url and \"news\" not in clean_url and \"occupazione-spazi\" not in clean_url and not re.search(r\"^https://corsi\\.unisa\\.it/\\d{5,}\", clean_url):\n",
    "                    should_add = True\n",
    "                elif new_domain == \"unisa.coursecatalogue.cineca.it\" and (current_domain == \"corsi.unisa.it\" or current_domain == \"unisa.coursecatalogue.cineca.it\") and clean_url != \"https://unisa.coursecatalogue.cineca.it/\" and \"gruppo\" not in clean_url and \"cerca-\" not in clean_url and \"support.apple.com\" not in clean_url and \"WWW.ESSE3WEB.UNISA.IT\" not in clean_url:\n",
    "                    signature = get_insegnamento_signature(param_cleaned_url)\n",
    "                    if not signature or signature in seen_insegnamenti_signatures:\n",
    "                        continue # Salta se la firma non è valida o è già stata vista\n",
    "\n",
    "                    seen_insegnamenti_signatures.add(signature)\n",
    "                    should_add = True\n",
    "                \n",
    "                if should_add:\n",
    "                    if param_cleaned_url not in visited_urls:\n",
    "                        new_path = current_path + [param_cleaned_url]\n",
    "                        urls_to_visit.append((param_cleaned_url, new_path))\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Errore durante la richiesta a {current_url}: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "print(\"\\nCrawling completato.\")\n",
    "print(f\"Pagine HTML trovate: {len(html_urls)}\")\n",
    "\n",
    "# Salva solo gli URL, uno per riga\n",
    "with open(\"urls_html6.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    for url in sorted(list(html_urls)):\n",
    "        f.write(f\"{url}\\n\")\n",
    "\n",
    "print(\"File 'urls_html6.txt' salvato con la lista degli URL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pulizia lista link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letti 3501 URL da 'urls_html6_2.txt'.\n",
      "Processo completato. Salvati 2551 URL unici in 'urls_html6_3.txt'.\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "# --- 1. CONFIGURAZIONE ---\n",
    "INPUT_FILE = \"urls_html6_2.txt\" \n",
    "OUTPUT_FILE = \"urls_html6_3.txt\"\n",
    "\n",
    "# --- 2. FUNZIONI DI SUPPORTO ---\n",
    "\n",
    "def get_insegnamento_signature(url):\n",
    "    \"\"\"\n",
    "    Se l'URL è di tipo 'insegnamenti', calcola la sua \"firma\" unica\n",
    "    rimuovendo il penultimo segmento del percorso. Altrimenti, restituisce None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path_segments = urlparse(url).path.strip('/').split('/')\n",
    "        if len(path_segments) < 2:\n",
    "            return None\n",
    "        signature = tuple(path_segments[:-2] + path_segments[-1:])\n",
    "        return signature\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def filter_and_deduplicate_urls(input_filepath, output_filepath):\n",
    "    \"\"\"\n",
    "    Legge gli URL, rimuove quelli contenenti 'rubrica.unisa.it', de-duplica\n",
    "    i link 'insegnamenti' simili e salva il risultato.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as f:\n",
    "            all_urls = [line.strip() for line in f if line.strip()]\n",
    "        print(f\"Letti {len(all_urls)} URL da '{input_filepath}'.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Errore: File di input '{input_filepath}' non trovato.\")\n",
    "        return\n",
    "\n",
    "    filtered_urls = []\n",
    "    seen_signatures = set()\n",
    "\n",
    "    for url in all_urls:\n",
    "\n",
    "        if \"rubrica.unisa.it\" in url:\n",
    "            continue\n",
    "\n",
    "        # Controlla se è un URL 'insegnamenti' per la de-duplicazione speciale\n",
    "        if \"unisa.coursecatalogue.cineca.it/insegnamenti\" in url:\n",
    "            signature = get_insegnamento_signature(url)\n",
    "            \n",
    "            if not signature or signature in seen_signatures:\n",
    "                continue\n",
    "            \n",
    "            seen_signatures.add(signature)\n",
    "            filtered_urls.append(url)\n",
    "        else:\n",
    "            # Se non è un URL da escludere o un 'insegnamenti' duplicato, lo teniamo\n",
    "            filtered_urls.append(url)\n",
    "\n",
    "    # Scrive i risultati nel file di output\n",
    "    try:\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "            for url in filtered_urls:\n",
    "                f.write(f\"{url}\\n\")\n",
    "        \n",
    "        print(f\"Processo completato. Salvati {len(filtered_urls)} URL unici in '{output_filepath}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il salvataggio del file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_and_deduplicate_urls(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- 1. CONFIGURAZIONE ---\n",
    "\n",
    "# File dove salvare lo stato delle pagine (ETag, Last-Modified, e hash)\n",
    "STATE_FILE = \"page_update_state.json\"\n",
    "\n",
    "# Header da inviare per simulare un browser reale\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# --- 2. FUNZIONI DI SUPPORTO ---\n",
    "\n",
    "def load_state(filepath):\n",
    "    \"\"\"Carica in modo sicuro lo stato precedente da un file JSON.\"\"\"\n",
    "    if not os.path.exists(filepath) or os.path.getsize(filepath) == 0:\n",
    "        return {}\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except (json.JSONDecodeError, IOError) as e:\n",
    "        print(f\"Attenzione: impossibile leggere il file di stato '{filepath}'. Parto da uno stato vuoto. Errore: {e}\")\n",
    "        return {}\n",
    "\n",
    "def save_state(filepath, state):\n",
    "    \"\"\"Salva lo stato corrente in un file JSON.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(state, f, indent=4)\n",
    "        print(f\"\\nStato aggiornato salvato con successo in '{filepath}'.\")\n",
    "    except IOError as e:\n",
    "        print(f\"Errore critico: impossibile salvare il file di stato '{filepath}'. Errore: {e}\")\n",
    "\n",
    "def get_content_hash(content: str) -> str:\n",
    "    \"\"\"Calcola l'hash SHA256 del contenuto testuale di una pagina.\"\"\"\n",
    "    return hashlib.sha256(content.encode('utf-8')).hexdigest()\n",
    "\n",
    "# --- 3. LOGICA PRINCIPALE DI CONTROLLO ---\n",
    "\n",
    "def check_for_updates_robust(urls_to_check, last_state):\n",
    "    \"\"\"\n",
    "    Controlla una lista di URL usando una strategia ibrida basata solo su richieste GET.\n",
    "    \"\"\"\n",
    "    updated_urls = []\n",
    "    current_state = {}\n",
    "    \n",
    "    print(f\"Controllo di {len(urls_to_check)} URL per aggiornamenti...\")\n",
    "    \n",
    "    for url in urls_to_check:\n",
    "        print(f\"\\n-> Controllando: {url}\")\n",
    "        previous_data = last_state.get(url, {})\n",
    "        request_headers = HEADERS.copy()\n",
    "\n",
    "        # Aggiungi gli header di caching se li abbiamo salvati\n",
    "        if previous_data.get(\"etag\"):\n",
    "            request_headers[\"If-None-Match\"] = previous_data[\"etag\"]\n",
    "        if previous_data.get(\"last_modified\"):\n",
    "            request_headers[\"If-Modified-Since\"] = previous_data[\"last_modified\"]\n",
    "\n",
    "        try:\n",
    "            # Il 'with' assicura che la connessione sia chiusa correttamente.\n",
    "            with requests.get(url, headers=request_headers, timeout=10, allow_redirects=True, stream=True) as response:\n",
    "                time.sleep(0.5)\n",
    "\n",
    "                # 1. CONTROLLO EFFICIENTE TRAMITE HEADER\n",
    "                if response.status_code == 304: # 304 Not Modified\n",
    "                    print(\"   Stato: Non modificato (304 via GET).\")\n",
    "                    current_state[url] = previous_data\n",
    "                    continue\n",
    "                \n",
    "                # Se il server risponde 200 OK e fornisce header di caching, li usiamo\n",
    "                # senza scaricare l'intero contenuto.\n",
    "                if response.status_code == 200 and (response.headers.get(\"ETag\") or response.headers.get(\"Last-Modified\")):\n",
    "                    print(\"   Stato: Aggiornato (rilevato via header GET). Salvo nuovi ETag/Last-Modified.\")\n",
    "                    updated_urls.append(url)\n",
    "                    current_state[url] = {\n",
    "                        \"etag\": response.headers.get(\"ETag\"),\n",
    "                        \"last_modified\": response.headers.get(\"Last-Modified\"),\n",
    "                        \"content_hash\": None # Resettiamo l'hash\n",
    "                    }\n",
    "                    continue\n",
    "\n",
    "                # 2. FALLBACK SU HASH DEL CONTENUTO\n",
    "                # Solo se il server risponde 200 OK ma non fornisce header di caching,\n",
    "                # procediamo a scaricare l'intero contenuto.\n",
    "                if response.status_code == 200:\n",
    "                    print(\"   Info: Il server non supporta caching efficiente. Eseguo fallback su hash del contenuto.\")\n",
    "                    \n",
    "                    # Scarica il contenuto del body\n",
    "                    content = response.text\n",
    "                    new_hash = get_content_hash(content)\n",
    "                    old_hash = previous_data.get(\"content_hash\")\n",
    "                    \n",
    "                    if new_hash != old_hash:\n",
    "                        print(f\"   Stato: Aggiornato (rilevato via hash). Hash: {new_hash[:10]}... (precedente: {str(old_hash)[:10]}...)\")\n",
    "                        updated_urls.append(url)\n",
    "                        current_state[url] = {\n",
    "                            \"etag\": None,\n",
    "                            \"last_modified\": None,\n",
    "                            \"content_hash\": new_hash\n",
    "                        }\n",
    "                    else:\n",
    "                        print(\"   Stato: Non modificato (hash identico).\")\n",
    "                        current_state[url] = previous_data\n",
    "                else:\n",
    "                    # Gestisce altri status code (es. 403, 404, 500)\n",
    "                    response.raise_for_status()\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"   ERRORE: Impossibile controllare l'URL. Errore: {e}\")\n",
    "            if url in last_state:\n",
    "                current_state[url] = last_state[url]\n",
    "            \n",
    "    return updated_urls, current_state\n",
    "\n",
    "# --- 4. ESECUZIONE ---\n",
    "url_filepath = \"urls_html6_3.txt\"\n",
    "\n",
    "with open(url_filepath, \"r\") as f:\n",
    "    html_urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "last_known_state = load_state(STATE_FILE)\n",
    "updated_pages, new_state = check_for_updates_robust(html_urls, last_known_state)\n",
    "\n",
    "if updated_pages:\n",
    "    print(\"\\n--- Pagine Aggiornate Rilevate ---\")\n",
    "    for page in updated_pages:\n",
    "        print(f\"- {page}\")\n",
    "else:\n",
    "    print(\"\\n--- Nessuna Pagina Aggiornata Rilevata ---\")\n",
    "    \n",
    "save_state(STATE_FILE, new_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "from pypdf import PdfReader\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "def process_pdfs(url_list_file, url_list_downloaded_file):\n",
    "    \"\"\"\n",
    "    Legge gli URL dei PDF, li scarica in memoria, estrae il testo \n",
    "    e restituisce una lista di Document di LlamaIndex.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(url_list_file, \"r\") as f:\n",
    "        urls_to_process = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"Trovati {len(urls_to_process)} PDF da processare in memoria.\")\n",
    "\n",
    "    with open(url_list_downloaded_file, \"r\") as f:\n",
    "        already_downloaded_urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    pdfs_to_process = [url for url in urls_to_process if url not in already_downloaded_urls]\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    pdf_documents = []\n",
    "\n",
    "    for url in pdfs_to_process:\n",
    "        try:\n",
    "            print(f\"-> Processando in memoria: {url}\")\n",
    "            response = requests.get(url, timeout=30, headers=headers)\n",
    "            response.raise_for_status() # Controlla errori HTTP\n",
    "\n",
    "            # Assicurati che sia un PDF prima di continuare\n",
    "            if 'application/pdf' not in response.headers.get('content-type', ''):\n",
    "                print(f\"  [SKIPPATO] L'URL non è un PDF, ma: {response.headers.get('content-type')}\")\n",
    "                continue\n",
    "\n",
    "            pdf_bytes = io.BytesIO(response.content)\n",
    "            \n",
    "            reader = PdfReader(pdf_bytes)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() or \"\" # Aggiungi \"\" se la pagina è vuota\n",
    "            \n",
    "            if not text.strip():\n",
    "                print(\"  [SKIPPATO] Il PDF è vuoto o contiene solo immagini (no testo).\")\n",
    "                continue\n",
    "\n",
    "            doc = Document(\n",
    "                text=text,\n",
    "                metadata={\n",
    "                    \"source_url\": url,\n",
    "                }\n",
    "            )\n",
    "            pdf_documents.append(doc)\n",
    "\n",
    "            # Aggiungi l'URL alla lista dei già scaricati\n",
    "            with open(url_list_downloaded_file, \"a\") as f:\n",
    "                f.write(url + \"\\n\")\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"  [ERRORE DOWNLOAD]: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERRORE LETTURA PDF]: {e}\")\n",
    "\n",
    "    return pdf_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTML Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "from llama_index.core.schema import Document\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def make_markdown_links_absolute(markdown_text, base_url):\n",
    "    \"\"\"\n",
    "    Trova tutti i link in formato Markdown [testo](url) in una stringa\n",
    "    e converte gli URL relativi in URL assoluti.\n",
    "    \"\"\"\n",
    "    \n",
    "    def replacer(match):\n",
    "        # Estrai il testo del link (gruppo 1) e l'URL (gruppo 2) dal match\n",
    "        link_text = match.group(1)\n",
    "        link_url = match.group(2)\n",
    "        \n",
    "        # Trasforma l'URL in assoluto. Se è già assoluto, non cambia nulla.\n",
    "        absolute_url = urljoin(base_url, link_url)\n",
    "        \n",
    "        # Ricostruisci il link in formato Markdown\n",
    "        return f\"[{link_text}]({absolute_url})\"\n",
    "\n",
    "    # L'espressione regolare per trovare tutti i link Markdown\n",
    "    # Cerca un pattern [qualsiasi_testo](qualsiasi_url)\n",
    "    markdown_link_pattern = r'\\[([^\\]]+)\\]\\(([^)]+)\\)'\n",
    "    \n",
    "    # Usa re.sub con la funzione replacer per sostituire tutti i link trovati\n",
    "    return re.sub(markdown_link_pattern, replacer, markdown_text)\n",
    "\n",
    "# --- ESECUZIONE CODICE ---\n",
    "\n",
    "url_filepath = \"urls_html_master_list.txt\"\n",
    "\n",
    "with open(url_filepath, \"r\") as f:\n",
    "    html_urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(\"Fase 1: Isolamento del blocco HTML principale...\")\n",
    "\n",
    "from MCER import MainContentExtractorReader\n",
    "loader = MainContentExtractorReader()\n",
    "\n",
    "html_documents = loader.load_data(urls=html_urls)\n",
    "\n",
    "try:\n",
    "\n",
    "    # Controlla che il numero di documenti e di URL corrisponda\n",
    "    if len(html_documents) != len(html_urls):\n",
    "        print(\"!!! ATTENZIONE: Il numero di documenti processati non corrisponde al numero di URL.\")\n",
    "        print(f\"Documenti: {len(html_documents)}, URL: {len(html_urls)}\")\n",
    "    else:\n",
    "        # Itera contemporaneamente su documenti e URL e assegna i metadati\n",
    "        for doc, url in zip(html_documents, html_urls):\n",
    "            doc.metadata[\"source_url\"] = url\n",
    "        \n",
    "        print(f\"Metadato 'source_url' aggiunto con successo a {len(html_documents)} documenti.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRORE: File '{url_filepath}' non trovato. Assicurati che il percorso sia corretto.\")\n",
    "\n",
    "# Processa ogni blocco di HTML per ottenere il testo finale\n",
    "print(\"Fase 2: Elaborazione dell'HTML per incorporare i link...\")\n",
    "processed_documents = []\n",
    "for doc in html_documents:\n",
    "\n",
    "    base_url = doc.metadata.get(\"source_url\", \"\")\n",
    "\n",
    "    clean_text_with_links = make_markdown_links_absolute(doc.text, base_url)\n",
    "    \n",
    "    # Crea un nuovo oggetto Document con il testo finale e i metadati originali\n",
    "    processed_documents.append(\n",
    "        Document(text=clean_text_with_links, metadata=doc.metadata)\n",
    "    )\n",
    "\n",
    "print(\"\\n--- ESEMPIO DI OUTPUT ---\")\n",
    "if processed_documents:\n",
    "    save_to_pickle(processed_documents, \"processed_documents7.pkl\")\n",
    "    print(processed_documents[0].text[:500])  # Stampa i primi 500 caratteri del testo del primo documento\n",
    "    print(processed_documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parser, splitter and metadata extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "# --- 1. CONFIGURAZIONE ---\n",
    "\n",
    "LLM_MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "MIN_DELAY_SECONDS = 1\n",
    "MAX_DELAY_SECONDS = 2\n",
    "\n",
    "# Indice di partenza per la ripresa del processo\n",
    "START_INDEX = 372\n",
    "\n",
    "# File di input e output\n",
    "DOCUMENTS_PICKLE_FILE = \"processed_documents7.pkl\" \n",
    "OUTPUT_METADATA_FILE = \"data/extracted_metadata.json\"\n",
    "\n",
    "# --- 2. INIZIALIZZAZIONE E CARICAMENTO DATI ---\n",
    "\n",
    "gemini_model = GoogleGenAI(model=LLM_MODEL_NAME)\n",
    "\n",
    "try:\n",
    "    original_documents = load_from_pickle(DOCUMENTS_PICKLE_FILE)\n",
    "except Exception as e:\n",
    "    print(f\"ERRORE CRITICO: Impossibile caricare i documenti di contesto. {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 3. FUNZIONE DI ESTRAZIONE METADATI ---\n",
    "\n",
    "def extract_metadata_single_doc(document_text, model):\n",
    "    \"\"\"\n",
    "    Chiama l'LLM con un prompt specifico per estrarre tutti i metadati\n",
    "    in un unico output formattato come JSON.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = (\n",
    "            \"Analizza il seguente documento, inclusa la sua URL di origine, per estrarre i metadati richiesti. \"\n",
    "            \"Fornisci l'output esclusivamente in formato JSON, seguendo la struttura e le regole specificate.\\n\\n\"\n",
    "            \"--- DOCUMENTO ---\\n\"\n",
    "            f'\"\"\"{document_text}\"\"\"\\n'\n",
    "            \"--- FINE DOCUMENTO ---\\n\\n\"\n",
    "            \n",
    "            \"REGOLE GENERALI:\\n\"\n",
    "            \"- Se il documento è troppo breve o non contiene informazioni sufficienti per generare un campo specifico (title, summary, etc.), lascia quel campo vuoto (es. `\\\"title\\\": \\\"\\\"` o `\\\"questions\\\": []`).\\n\"\n",
    "            \"- **Keywords**: Estrai un numero di parole chiave proporzionale alla lunghezza del testo, fino a un massimo di 10. Per documenti molto brevi, poche parole chiave (o nessuna) sono accettabili.\\n\"\n",
    "            \"- **Domande**: Genera un numero di domande proporzionale alla lunghezza del testo, fino a un massimo di 3. Per documenti molto brevi, una sola domanda o nessuna sono accettabili.\\n\\n\"\n",
    "\n",
    "            \"REGOLE PER 'years':\\n\"\n",
    "            \"- Identifica l'anno o gli anni accademici (es. '2024/2025') o solari (es. '2023') *PRINCIPALI* del documento. Controlla sia il testo che l'URL di origine.\\n\"\n",
    "            \"- Se il documento tratta un singolo anno, inserisci solo quello. Esempio: [\\\"2023\\\"].\\n\"\n",
    "            \"- Se nell'URL è presente il parametro 'anno', considera il suo valore corrispondente come UNICO anno principale, NON considerare il parametro quando 'anno=0'.\\n\"\n",
    "            \"- Se tratta più anni, IDENTIFICA QUELLO PRINCIPALE ed inseriscilo, altrimenti inseriscili tutti. Esempio: [\\\"2022\\\", \\\"2023\\\", \\\"2024\\\"].\\n\"\n",
    "            \"- Se non riesci ad identificare l'anno principale dal testo, ma è presente nell'URL, usa quello. Esempio: [\\\"2023\\\"] se l'URL contiene 'anno=2023' (sempre escludendo il caso in cui sia 'anno=0').\\n\"\n",
    "            \"- Se non ha un anno di riferimento, lascia la lista vuota. Esempio: [].\\n\\n\"\n",
    "\n",
    "            \"Formato JSON richiesto:\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \"title\": \"Un titolo conciso e descrittivo del documento\",\\n'\n",
    "            '  \"summary\": \"Un riassunto di 2-3 frasi del contenuto principale\",\\n'\n",
    "            '  \"questions\": [],\\n'  # Da 0 a 3 domande in base alla lunghezza del testo\n",
    "            '  \"keywords\": [],\\n'   # Da 0 a 10 parole chiave in base alla lunghezza del testo\n",
    "            '  \"years\": []\\n'\n",
    "            \"}\\n\\n\"\n",
    "            \"Output JSON:\"\n",
    "        )\n",
    "        response = model.complete(prompt)\n",
    "        # Rimuovi eventuali ```json ... ``` che il modello potrebbe aggiungere\n",
    "        cleaned_response = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        return cleaned_response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Si è verificato un errore durante la chiamata all'LLM: {e}\")\n",
    "        raise ConnectionError(f\"La chiamata API è fallita con errore: {e}\")\n",
    "\n",
    "# --- 4. FLUSSO PRINCIPALE ---\n",
    "\n",
    "def extract_metadata():\n",
    "\n",
    "    gemini_model =  GoogleGenAI(model=LLM_MODEL_NAME, temperature=0.2)\n",
    "\n",
    "    documents_to_process = original_documents[START_INDEX:]\n",
    "    all_extracted_metadata = []\n",
    "    \n",
    "    print(f\"Inizio estrazione metadati per {len(documents_to_process)} documenti (partendo dall'indice {START_INDEX})...\")\n",
    "\n",
    "    try:\n",
    "        for i, document in enumerate(tqdm(documents_to_process, desc=\"Estraendo metadati\"), start=START_INDEX):\n",
    "\n",
    "            if not isinstance(document.text, str) or not document.text.strip():\n",
    "                print(f\"\\nDocumento {i} saltato perché vuoto.\")\n",
    "                continue\n",
    "\n",
    "            # Estrai i metadati come stringa JSON\n",
    "            json_string_response = extract_metadata_single_doc(document.text, gemini_model)\n",
    "\n",
    "            # Prova a parsare la stringa JSON per ottenere un dizionario\n",
    "            try:\n",
    "                metadata = json.loads(json_string_response)\n",
    "                # Aggiungi informazioni utili per il tracciamento\n",
    "                metadata[\"original_document_index\"] = i\n",
    "                metadata[\"source_url\"] = document.metadata.get(\"source_url\", \"N/A\")\n",
    "                all_extracted_metadata.append(metadata)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"\\nATTENZIONE: Risposta non JSON valida per il documento {i}. Risposta ricevuta:\\n{json_string_response[:200]}...\")\n",
    "                # Salva comunque la risposta grezza per un'analisi successiva\n",
    "                all_extracted_metadata.append({\n",
    "                    \"original_document_index\": i,\n",
    "                    \"source_url\": document.metadata.get(\"source_url\", \"N/A\"),\n",
    "                    \"error\": \"Invalid JSON response\",\n",
    "                    \"raw_response\": json_string_response\n",
    "                })\n",
    "\n",
    "            # Pausa per rispettare i rate limit\n",
    "            sleep_time = random.uniform(MIN_DELAY_SECONDS, MAX_DELAY_SECONDS)\n",
    "            time.sleep(sleep_time)\n",
    "            \n",
    "    finally:\n",
    "        # Questo blocco viene eseguito SEMPRE, sia in caso di successo che di errore.\n",
    "        print(\"\\n--- Blocco di salvataggio finale in esecuzione ---\")\n",
    "        if not all_extracted_metadata:\n",
    "            print(\"Nessun nuovo metadato da salvare in questa sessione.\")\n",
    "        else:\n",
    "            try:\n",
    "                existing_data = []\n",
    "                # Controlla se dobbiamo aggiungere a un file esistente\n",
    "                if START_INDEX > 0 and os.path.exists(OUTPUT_METADATA_FILE):\n",
    "                    print(f\"Tentativo di aggiungere i nuovi risultati al file esistente '{OUTPUT_METADATA_FILE}'...\")\n",
    "                    with open(OUTPUT_METADATA_FILE, 'r', encoding='utf-8') as f:\n",
    "                        try:\n",
    "                            # Carica i metadati già salvati\n",
    "                            existing_data = json.load(f)\n",
    "                            print(f\"Caricati {len(existing_data)} record esistenti.\")\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(\"ATTENZIONE: Il file di output esisteva ma era corrotto. Verrà sovrascritto.\")\n",
    "\n",
    "                # Unisci i dati vecchi e quelli nuovi\n",
    "                final_data = existing_data + all_extracted_metadata\n",
    "\n",
    "                # Scrivi il file completo\n",
    "                with open(OUTPUT_METADATA_FILE, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(final_data, f, ensure_ascii=False, indent=4)\n",
    "                \n",
    "                print(f\"Salvataggio completato! Totale di {len(final_data)} record di metadati in '{OUTPUT_METADATA_FILE}'.\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"ERRORE durante il salvataggio del file JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# --- 1. CONFIGURAZIONE ---\n",
    "\n",
    "DOCUMENTS_PICKLE_FILE = \"processed_documents7.pkl\"\n",
    "\n",
    "METADATA_FILE = \"data/extracted_metadata.json\"\n",
    "\n",
    "OUTPUT_PICKLE_FILE = \"processed_documents7.pkl\"\n",
    "\n",
    "# --- 2. FUNZIONI DI CARICAMENTO E SALVATAGGIO ---\n",
    "\n",
    "def load_metadata_from_json(filepath):\n",
    "    \"\"\"Carica una lista di metadati da un file JSON.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            metadata_list = json.load(f)\n",
    "        print(f\"Caricati {len(metadata_list)} record di metadati da '{filepath}'.\")\n",
    "        return metadata_list\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERRORE: File dei metadati '{filepath}' non trovato.\")\n",
    "        raise\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"ERRORE: Il file '{filepath}' non è un JSON valido.\")\n",
    "        raise\n",
    "\n",
    "# --- 3. FLUSSO PRINCIPALE ---\n",
    "\n",
    "def add_metadata():\n",
    "    \"\"\"\n",
    "    Carica documenti e metadati, li unisce e salva il risultato.\n",
    "    \"\"\"\n",
    "    # Carica i dati necessari\n",
    "    documents = load_from_pickle(DOCUMENTS_PICKLE_FILE)\n",
    "    metadata_list = load_metadata_from_json(METADATA_FILE)\n",
    "    \n",
    "    print(\"\\nInizio processo di assegnazione dei metadati...\")\n",
    "    \n",
    "    assigned_count = 0\n",
    "    # Itera su ogni record di metadati estratto\n",
    "    for metadata_record in metadata_list:\n",
    "        # Controllo se il record contiene l'indice del documento originale\n",
    "        if \"original_document_index\" not in metadata_record:\n",
    "            print(f\"ATTENZIONE: Record di metadati saltato perché non contiene 'original_document_index'.\")\n",
    "            continue\n",
    "            \n",
    "        doc_index = metadata_record.pop(\"original_document_index\")\n",
    "        \n",
    "        # Controllo che l'indice sia valido per la lista dei documenti\n",
    "        if 0 <= doc_index < len(documents):\n",
    "            # Prendi il documento corrispondente\n",
    "            target_document = documents[doc_index]\n",
    "            \n",
    "            # Aggiorna il dizionario dei metadati del documento\n",
    "            target_document.metadata.update(metadata_record)\n",
    "            assigned_count += 1\n",
    "        else:\n",
    "            print(f\"ATTENZIONE: Indice documento '{doc_index}' non valido. Metadato saltato.\")\n",
    "            \n",
    "    print(f\"\\nAssegnazione completata. Aggiornati i metadati per {assigned_count} documenti.\")\n",
    "    \n",
    "    # Salva la lista di documenti ora arricchita in un nuovo file pickle\n",
    "    save_to_pickle(documents, OUTPUT_PICKLE_FILE)\n",
    "\n",
    "add_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "markdown_splitter = MarkdownNodeParser()\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "semantic_splitter = SemanticSplitterNodeParser(\n",
    "    embed_model=embed_model,\n",
    "    buffer_size=1, \n",
    "    breakpoint_percentile_threshold=98,\n",
    ")\n",
    "\n",
    "hierarchical_parser = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[4096*4, 1024*4, 512*2],\n",
    "    chunk_overlap=64\n",
    ")\n",
    "\n",
    "sentence_splitter = SentenceSplitter(\n",
    "    chunk_size=512*16,\n",
    "    chunk_overlap=512*2\n",
    ")\n",
    "\n",
    "sentence_splitter2 = SentenceSplitter(\n",
    "    chunk_size=512*4,\n",
    "    chunk_overlap=256 \n",
    ")\n",
    "\n",
    "sentence_window_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    sentence_splitter=sentence_splitter2,\n",
    "    # how many sentences on either side to capture\n",
    "    window_size=3,\n",
    "    # the metadata key that holds the window of surrounding sentences\n",
    "    window_metadata_key=\"window\",\n",
    "    # the metadata key that holds the original sentence\n",
    "    original_text_metadata_key=\"original_sentence\",\n",
    ")\n",
    "\n",
    "transformations = [\n",
    "    # markdown_splitter,\n",
    "    sentence_splitter,\n",
    "    # hierarchical_parser,\n",
    "    # semantic_splitter,\n",
    "    # sentence_window_parser\n",
    "]\n",
    "\n",
    "# Pipeline per i documenti HTML\n",
    "pipeline = IngestionPipeline(transformations=transformations)\n",
    "processed_documents = load_from_pickle(\"documents/processed_documents7.pkl\")\n",
    "nodes = pipeline.run(documents=processed_documents, show_progress=True)\n",
    "save_to_pickle(nodes, \"nodes/nodes_metadata_sentence_x16.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_and_plot_nodes(nodes):\n",
    "    \"\"\"\n",
    "    Analizza una lista di nodi, stampa statistiche sulla loro lunghezza\n",
    "    e genera un istogramma della distribuzione delle lunghezze.\n",
    "    \"\"\"\n",
    "    if not nodes:\n",
    "        print(\"La lista dei nodi è vuota. Impossibile analizzare.\")\n",
    "        return\n",
    "\n",
    "    # Calcola le lunghezze dei nodi\n",
    "    node_lengths = [len(node.get_content()) for node in nodes]\n",
    "    \n",
    "    # --- Calcola Statistiche ---\n",
    "    avg_length = int(sum(node_lengths) / len(nodes))\n",
    "    max_length = max(node_lengths)\n",
    "    total_nodes = len(nodes)\n",
    "\n",
    "    print(f\"--- Analisi dei Nodi ---\")\n",
    "    print(f\"Numero totale di nodi: {total_nodes}\")\n",
    "    print(f\"Lunghezza massima di un nodo (in caratteri): {max_length}\")\n",
    "    print(f\"Lunghezza media di un nodo (in caratteri): {avg_length}\")\n",
    "\n",
    "    # --- Creazione del Grafico ---\n",
    "    print(\"\\nGenerazione del grafico di distribuzione...\")\n",
    "    plt.figure(figsize=(10, 6)) # Imposta la dimensione del grafico\n",
    "    \n",
    "    # Crea l'istogramma e ottieni l'asse (ax) per la personalizzazione\n",
    "    ax = sns.histplot(data=node_lengths, kde=True, bins=50)\n",
    "    \n",
    "    # Aggiungi una linea verticale per la media\n",
    "    ax.axvline(x=avg_length, color='r', linestyle='--', label=f'Mean: {avg_length}')\n",
    "    \n",
    "    stats_text = (\n",
    "        f\"Total nodes: {total_nodes}\\n\"\n",
    "        f\"Max lenght (in characters): {max_length}\"\n",
    "    )\n",
    "    \n",
    "    # Posiziona il testo nell'angolo in alto a destra\n",
    "    ax.text(0.95, 0.95, stats_text,\n",
    "            transform=ax.transAxes,\n",
    "            verticalalignment='top',\n",
    "            horizontalalignment='right',\n",
    "            fontsize=12,\n",
    "            # Aggiunge un riquadro bianco semitrasparente per la leggibilità\n",
    "            bbox=dict(boxstyle='round,pad=0.5', fc='white', alpha=0.7))\n",
    "    \n",
    "    # Aggiungi etichette e titolo usando l'oggetto 'ax'\n",
    "    ax.set_title('Nodes Lenght Distribution')\n",
    "    ax.set_xlabel('Node Length (in characters)')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Mostra il grafico\n",
    "    plt.show()\n",
    "\n",
    "    # --- Ispezione dei Nodi più Lunghi ---\n",
    "    longest_nodes = sorted(nodes, key=lambda x: len(x.get_content()), reverse=True)\n",
    "    print(\"\\n--- Contenuto dei 5 nodi più lunghi ---\")\n",
    "    for i in range(min(5, len(longest_nodes))):\n",
    "        node = longest_nodes[i]\n",
    "        print(f\"Nodo {i+1}, Lunghezza: {len(node.get_content())}, Metadati: {node.metadata}\")\n",
    "        print(f\"Inizio del testo: {node.get_content()[:500]}...\\n\") # Stampa i primi 500 caratteri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import get_leaf_nodes\n",
    "\n",
    "nodes = load_from_pickle(\"nodes/nodes_metadata_hierarchical_x8x2x1.pkl\")\n",
    "# leaf_nodes = get_leaf_nodes(nodes)\n",
    "\n",
    "analyze_and_plot_nodes(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QDrant Vectore Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://e542824d-6590-4005-91db-6dd34bf8f471.eu-west-2-0.aws.cloud.qdrant.io:6333\", \n",
    ")\n",
    "\n",
    "qdrant_aclient = AsyncQdrantClient(\n",
    "    url=\"https://e542824d-6590-4005-91db-6dd34bf8f471.eu-west-2-0.aws.cloud.qdrant.io:6333\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    \"diem_chatbot3\",\n",
    "    client=qdrant_client,\n",
    "    aclient=qdrant_aclient,\n",
    ")\n",
    "\n",
    "# diem_chatbot1: nodes_metadata_sentence_x8.pkl\n",
    "# diem_chatbot2: nodes_metadata_hierarchical_x32x8x2.pkl\n",
    "# diem_chatbot3: nodes_metadata_sentence_x16.pkl\n",
    "# diem_chatbot4: nodes_metadata_hierarchical_x16x4x1.pkl\n",
    "# diem_chatbot5: nodes_metadata_hierarchical_x8x2x1.pkl\n",
    "# diem_chatbot_public: nodes_metadata_sentence_x16.pkl, con text-embeddings-004 (tra pochi mesi non più supportato)\n",
    "# diem_chatbot_final: nodes_metadata_sentence_x16.pkl, con gemini-embeddings-001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vectore store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import get_leaf_nodes\n",
    "\n",
    "nodes = load_from_pickle(\"nodes/nodes_metadata_hierarchical_x8x2x1.pkl\")\n",
    "leaf_nodes = get_leaf_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "\n",
    "# Creazione dello storage context e dell'indice\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Per sentence splitter\n",
    "vector_index = VectorStoreIndex(nodes, storage_context=storage_context, show_progress=True)\n",
    "# Per hierarchical splitter\n",
    "vector_index = VectorStoreIndex(leaf_nodes, storage_context=storage_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load vectore store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentence splitter case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hierarchical splitter case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "\n",
    "# Insert nodes into docstore\n",
    "docstore.add_documents(nodes)\n",
    "\n",
    "# Define storage context\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query/Chat engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_template = (\n",
    "    \"\"\"Sei AskDIEM, un assistente virtuale dell'Università di Salerno, specializzato nell'aiutare gli studenti del Dipartimento di Ingegneria dell'Informazione ed Elettrica e Matematica Applicata (DIEM).\n",
    "\n",
    "    Il tuo obiettivo è fornire risposte accurate basandoti esclusivamente sulle informazioni ufficiali che ti vengono fornite.\n",
    "    Tieni presente che oggi è: {current_date}.\n",
    "\n",
    "    REGOLE GENERALI:\n",
    "    - *IMPORTANTE*: Se la domanda ti viene posta in inglese rispondi in inglese, a prescindere dalla lingua dei messaggi precedenti o da quella del contesto fornito.\n",
    "    - A meno che nella domanda non venga specificato un anno o una data in particolare, rispondi sempre tenendo presente la data di oggi.\n",
    "    - Se nomini un evento, adegua i tempi verbali in base alla data attuale.\n",
    "    - Se non disponi delle informazioni necessarie per rispondere a una domanda, dichiara chiaramente: \"Non dispongo delle informazioni necessarie per rispondere a questa domanda.\"\n",
    "    - Non inventare mai informazioni, contatti, date o procedure. La tua priorità è l'accuratezza.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from babel.dates import format_datetime\n",
    "\n",
    "current_date_str = format_datetime(datetime.datetime.now(), format=\"EEEE, d MMMM yyyy\", locale=\"it_IT\")\n",
    "\n",
    "system_prompt = system_prompt_template.format(current_date=current_date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_prompt = (\n",
    "    \"\"\"Date le seguenti informazioni estratte dai documenti ufficiali e la domanda dell'utente, fornisci una risposta chiara ed esaustiva.\n",
    "\n",
    "    Contesto:\n",
    "    {context_str}\n",
    "\n",
    "    Istruzioni per la risposta:\n",
    "    - Se il contesto è presente ED È RILEVANTE per la domanda, basa la tua risposta su di esso.\n",
    "    - Se il contesto è vuoto o NON È RILEVANTE per la domanda (ad esempio, se la domanda è un saluto, \"come ti chiami?\", o una domanda conversazionale generica), rispondi alla domanda usando la tua conoscenza generale e seguendo la tua personalità definita nel system prompt.\n",
    "    \n",
    "    - ISTRUZIONE PER I LINK: Se nel contesto è presente una risorsa rilevante (come un PDF di un bando, una graduatoria o una pagina web) che supporta la tua risposta, devi citarla usando il formato Markdown: [Titolo Significativo](URL).\n",
    "    - Il \"Titolo Significativo\" dovrebbe essere il titolo del documento (es. 'Bando Collaborazioni studentesche 2024') che trovi nel contesto.\n",
    "    - L' \"URL\" è l'indirizzo web (source_url) associato a quel titolo.\n",
    "    \n",
    "    - Esempio di formato CORRETTO:\n",
    "    Per maggiori dettagli, puoi consultare il [Bando per Collaborazioni Studentesche](https://www.unisa.it/bando-collaborazioni-...).\n",
    "    \n",
    "    - Esempio di formato ERRATO (da non usare):\n",
    "    Per maggiori dettagli, puoi consultare https://www.unisa.it/bando-collaborazioni-...\n",
    "    \n",
    "    - Non includere link o titoli che non siano esplicitamente presenti nel contesto.\n",
    "\n",
    "    Domanda: {query_str}\n",
    "    Risposta:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexAutoRetriever\n",
    "from llama_index.core.vector_stores.types import MetadataInfo, VectorStoreInfo\n",
    "\n",
    "metadata_field_info = VectorStoreInfo(\n",
    "    content_info=\"Blocchi di testo estratti da documenti e pagine web dell'Università degli Studi di Salerno, riguardanti avvisi, offerta formativa, regolamenti e altre informazioni accademiche.\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"title\",\n",
    "            type=\"str\",\n",
    "            description=\"Il titolo completo del documento.\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"summary\",\n",
    "            type=\"str\",\n",
    "            description=\"Un riassunto conciso del contenuto principale e dello scopo del documento.\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"questions\",\n",
    "            type=\"list[str]\",\n",
    "            description=\"Una lista di domande specifiche a cui il testo del documento è in grado di rispondere.\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"keywords\",\n",
    "            type=\"list[str]\",\n",
    "            description=\"Una lista di parole chiave e concetti principali estratti dal documento. Utile per ricerche tematiche (es. 'seminario', 'ingegneria informatica').\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"years\",\n",
    "            type=\"list[str]\",\n",
    "            description=\"Una lista di anni accademici o solari di riferimento per il documento (es. '2026', '2027'). Cruciale per filtrare le informazioni per un anno specifico.\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"source_url\",\n",
    "            type=\"str\",\n",
    "            description=\"L'URL originale della pagina web da cui è stato estratto il documento.\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "retriever = VectorIndexAutoRetriever(\n",
    "    vector_index,\n",
    "    vector_store_info=metadata_field_info,\n",
    "    similarity_top_k=15,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "\n",
    "base_retriever = vector_index.as_retriever(similarity_top_k=15)\n",
    "\n",
    "retriever = AutoMergingRetriever(\n",
    "    base_retriever,\n",
    "    storage_context,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "qa_prompt_template_str = system_prompt + context_prompt\n",
    "qa_prompt_template = PromptTemplate(qa_prompt_template_str)\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"compact\", text_qa_template=qa_prompt_template)\n",
    "\n",
    "st_rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L6-v2\", top_n=15\n",
    ")\n",
    "\n",
    "similarity_pp = SimilarityPostprocessor(similarity_cutoff=0.15)\n",
    "\n",
    "# Query engine used only for evaluation\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=base_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[st_rerank, similarity_pp],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "import os\n",
    "\n",
    "cohere_rerank = CohereRerank(api_key=os.environ['COHERE_API_KEY'], top_n=15)\n",
    "\n",
    "chat_engine = CondensePlusContextChatEngine.from_defaults(\n",
    "    retriever=base_retriever,\n",
    "    memory=memory,\n",
    "    system_prompt=system_prompt,\n",
    "    context_prompt=context_prompt,\n",
    "    node_postprocessors=[cohere_rerank],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.stream_chat(\"Chi è mario vento?\")\n",
    "\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Documenti usati per generare la risposta:\")\n",
    "if response.source_nodes:\n",
    "    for i, node in enumerate(response.source_nodes):\n",
    "        print(f\"--- Documento Sorgente {i+1} (Score: {node.score:.2f}) ---\")\n",
    "        \n",
    "        if 'source_url' in node.metadata:\n",
    "            print(f\"URL: {node.metadata['source_url']}\")\n",
    "        else:\n",
    "            print(node.metadata)\n",
    "            print(\"Nessun URL disponibile per questo nodo.\")\n",
    "            \n",
    "        print(f\"Contenuto: {node.text[:500]}...\\n\")\n",
    "else:\n",
    "    print(\"Nessun documento sorgente è stato recuperato per questa query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Evaluation</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "eval_llm = GoogleGenAI(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    temperature=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "# --- 1. CONFIGURAZIONE ---\n",
    "\n",
    "# Modello LLM da utilizzare\n",
    "LLM_MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "# Numero di domande da generare per ogni documento\n",
    "QUESTIONS_PER_DOCUMENT = 2\n",
    "\n",
    "# Pausa tra le chiamate API (in secondi)\n",
    "MIN_DELAY_SECONDS = 1\n",
    "MAX_DELAY_SECONDS = 2\n",
    "\n",
    "# Indice di partenza per il ciclo sui documenti\n",
    "START_INDEX = 32\n",
    "\n",
    "# File di output per le domande generate\n",
    "OUTPUT_FILE = \"data/generated_questions3.json\"\n",
    "\n",
    "# --- 2. FUNZIONE DI GENERAZIONE ---\n",
    "\n",
    "def generate_questions_for_document(document_text, model, num_questions):\n",
    "    \"\"\"\n",
    "    Chiama l'LLM per generare un numero specifico di domande basate sul testo di un documento.\n",
    "    Restituisce una lista di domande.\n",
    "    \"\"\"\n",
    "    try:        \n",
    "        prompt = (\n",
    "            f\"Basandoti sul seguente documento, genera esattamente {num_questions} domande pertinenti e auto-contenute.\\n\\n\"\n",
    "            \"DOCUMENTO:\\n\"\n",
    "            f'\"\"\"{document_text}\"\"\"\\n\\n'\n",
    "            \n",
    "            \"ISTRUZIONI FONDAMENTALI:\\n\"\n",
    "            \"- **Crea domande auto-contenute**: Ogni domanda deve includere il soggetto principale del testo (es. il nome di un corso, di un regolamento, o di un professore) per essere comprensibile *ANCHE SENZA LEGGERE IL DOCUMENTO*.\\n\"\n",
    "            \"- **Esempi di domande da NON fare**:\\n\"\n",
    "            \"  1)\\\"In quale semestre si svolgerà il corso?\\\" (Non si sa a quale corso si fa riferimento)\\n\"\n",
    "            \"  2)\\\"In quale intervallo di date erano previste le immatricolazioni per il dottorato di ricerca in \\\"INGEGNERIA INDUSTRIALE\\\" secondo il Decreto Rettorale menzionato?\\\" (Non si sa cosa è stato menzionato)\\n\"\n",
    "            \"  3)\\\"Quale categoria di notizie è attualmente visualizzata nel documento, come indicato dal titolo \\\"Premi e riconoscimenti\\\"?\\\" (Non si sa a quale documento fa riferimento)\\n\"\n",
    "            \"  4)\\\"In quale anno accademico si riferisce il Piano di Studi del corso di Digital Health and Bioinformatic Engineering presentato in questa pagina?\\\" (Non si sa a quale pagina fa riferimento)\\n\"\n",
    "            \"- **Esempi di domande da FARE (auto-contenute)**:\\n\"\n",
    "            \"  1)\\\"In quale semestre si svolgerà il corso di 'Sistemi Informativi Sanitari'?\\\"\\n\"\n",
    "            \"  2)\\\"In quale piattaforma gli studenti devono compilare il Piano di Studi per il corso di Digital Health and Bioinformatic Engineering?\\\"\\n\"\n",
    "            \"  3)\\\"Chi è il responsabile del corso \\\"ELEMENTI DI BIOCHIMICA E MEDICINA DI LABORATORIO\\\" e quali sono le ore dedicate alle esercitazioni e alle lezioni rispettivamente?\\\"\\n\"\n",
    "            \"  4)\\\"Quali sono i compiti del Delegato alla mobilità internazionale del Dipartimento di eccellenza?\\\"\\n\"\n",
    "            \"  5)\\\"Qual è il numero di crediti formativi universitari (CFU) associati al corso \\\"DRAFTING OF THE DOCTORATE THESIS\\\"?\\\"\\n\\n\"\n",
    "\n",
    "            \"ISTRUZIONI DI FORMATTAZIONE:\\n\"\n",
    "            \"- Restituisci solo ed esclusivamente le domande.\\n\"\n",
    "            \"- Separa ogni domanda con un 'a capo'.\\n\"\n",
    "            \"- Non numerare le domande.\\n\"\n",
    "            \"- Non aggiungere testo introduttivo o conclusivo.\\n\"\n",
    "            \"- Se un documento è troppo breve o non contiene informazioni sufficienti per generare il numero richiesto di domande, genera soltanto la frase \\\"DOMANDE NON GENERATE\\\".\"\n",
    "        )\n",
    "        \n",
    "        response = model.complete(prompt)\n",
    "        \n",
    "        # Pulisce la risposta e la divide in singole domande\n",
    "        questions = [q.strip() for q in response.text.strip().split('\\n') if q.strip()]\n",
    "        return questions\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Si è verificato un errore durante la chiamata all'LLM: {e}\")\n",
    "        # Solleva un'eccezione per interrompere il ciclo e attivare il salvataggio\n",
    "        raise ConnectionError(f\"La chiamata API è fallita con errore: {e}\")\n",
    "\n",
    "# --- 4. FLUSSO PRINCIPALE ---\n",
    "\n",
    "def question_generator():\n",
    "    \"\"\"\n",
    "    Flusso di lavoro principale: carica i documenti, genera le domande e salva il risultato.\n",
    "    \"\"\"\n",
    "    gemini_model = GoogleGenAI(model=LLM_MODEL_NAME, temperature=0.2)\n",
    "\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    \n",
    "    # --- Inserisci qui i tuoi documenti ---\n",
    "    documents = load_from_pickle(\"processed_documents7.pkl\")\n",
    "    #documents = nodes[START_INDEX:]\n",
    "    documents = documents[START_INDEX:]\n",
    "    \n",
    "    generated_data = []\n",
    "\n",
    "    print(f\"Inizio generazione di {QUESTIONS_PER_DOCUMENT} domande per {len(documents)} documenti...\")\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        for doc_index, doc in enumerate(tqdm(documents, desc=\"Generando domande\", leave=False), start=START_INDEX):\n",
    "            \n",
    "            print(f\"\\nProcessando documento {doc_index + 1}/{len(documents)}...\")\n",
    "\n",
    "            # 1. Estrai l'URL dai metadati\n",
    "            source_url = doc.metadata.get(\"source_url\", \"Nessuna fonte specificata\")\n",
    "            \n",
    "            # 2. Crea la stringa combinata da passare all'LLM.\n",
    "            document_text_with_context = (\n",
    "                f\"URL di origine del documento: {source_url}\\n\\n\"\n",
    "                f\"--- TESTO DEL DOCUMENTO ---\\n\"\n",
    "                f\"{doc.text}\"\n",
    "            )\n",
    "            \n",
    "            # Genera le domande per il documento corrente\n",
    "            questions = generate_questions_for_document(document_text_with_context, gemini_model, QUESTIONS_PER_DOCUMENT)\n",
    "            \n",
    "            if not questions:\n",
    "                print(f\"  -> Nessuna domanda generata per il documento {doc_index + 1}.\")\n",
    "                \n",
    "            else:\n",
    "                # Aggiungi le domande generate alla lista dei risultati\n",
    "                for question_num, question_text in enumerate(questions, start=1):\n",
    "                    generated_data.append({\n",
    "                        \"document_index\": doc_index,\n",
    "                        \"question_number\": question_num,\n",
    "                        \"question\": question_text\n",
    "                    })\n",
    "                print(f\"  -> Generate {len(questions)} domande.\")\n",
    "            \n",
    "            # Pausa per rispettare i rate limit\n",
    "            sleep_time = random.uniform(MIN_DELAY_SECONDS, MAX_DELAY_SECONDS)\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    finally:\n",
    "        if not generated_data:\n",
    "            print(\"Nessuna domanda è stata generata. Controlla eventuali errori precedenti.\")\n",
    "        else:\n",
    "            # Salva i dati in un file JSON\n",
    "            try:\n",
    "                with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "                    json.dump(generated_data, f, ensure_ascii=False, indent=4)\n",
    "                print(f\"\\nOperazione completata! {len(generated_data)} domande salvate in '{OUTPUT_FILE}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Errore durante il salvataggio del file JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "# --- 1. CONFIGURAZIONE ---\n",
    "\n",
    "LLM_MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "MIN_DELAY_SECONDS = 1\n",
    "MAX_DELAY_SECONDS = 2\n",
    "\n",
    "START_INDEX = 1905\n",
    "\n",
    "DOCUMENTS_PICKLE_FILE = \"processed_documents7.pkl\" \n",
    "INPUT_QUESTIONS_FILE = \"data/generated_questions3.json\"\n",
    "OUTPUT_QA_FILE = \"data/generated_rag_answers2.json\"\n",
    "\n",
    "# --- 2. CARICAMENTO DATI ---\n",
    "\n",
    "try:\n",
    "    original_documents = load_from_pickle(DOCUMENTS_PICKLE_FILE)\n",
    "    print(f\"Caricati {len(original_documents)} documenti di contesto.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRORE CRITICO: Impossibile caricare i documenti di contesto. {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 3. FUNZIONE DI GENERAZIONE RISPOSTE ---\n",
    "\n",
    "def generate_rag_answer(question, context, model):\n",
    "    \"\"\"\n",
    "    Chiama un LLM per generare una risposta basandosi esclusivamente su domanda e contesto.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = (\n",
    "            \"Il tuo compito è rispondere alla domanda basandoti esclusivamente sul contesto fornito.\\n\"\n",
    "            \"Le risposte devono essere esclusivamente in italiano.\\n\"\n",
    "            \"Rispondi in modo completo ed esaustivo. Inizia direttamente con la risposta, senza frasi introduttive o commenti.\\n\"\n",
    "            \"Se la risposta non è presente nel contesto, rispondi con 'Le informazioni non sono presenti nel contesto fornito.'\\n\\n\"\n",
    "            \"--- CONTESTO ---\\n\"\n",
    "            f\"{context}\\n\"\n",
    "            \"--- FINE CONTESTO ---\\n\\n\"\n",
    "            f\"Domanda: \\\"{question}\\\"\\n\\n\"\n",
    "            \"Risposta:\"\n",
    "        )\n",
    "\n",
    "        response = model.complete(prompt)\n",
    "        return response.text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Si è verificato un errore durante la chiamata all'LLM: {e}\")\n",
    "        # Solleva un'eccezione per interrompere il ciclo e attivare il salvataggio\n",
    "        raise ConnectionError(f\"La chiamata API è fallita con errore: {e}\")\n",
    "\n",
    "# --- 4. FLUSSO PRINCIPALE ---\n",
    "\n",
    "def answer_generator():\n",
    "    \"\"\"\n",
    "    Flusso di lavoro principale: carica le domande, genera le risposte RAG e salva i risultati.\n",
    "    \"\"\"\n",
    "\n",
    "    gemini_model = GoogleGenAI(model=LLM_MODEL_NAME, temperature=0.2)\n",
    "\n",
    "    try:\n",
    "        with open(INPUT_QUESTIONS_FILE, 'r', encoding='utf-8') as f:\n",
    "            all_questions_data = json.load(f)\n",
    "        print(f\"Caricate {len(all_questions_data)} domande da '{INPUT_QUESTIONS_FILE}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERRORE: Impossibile caricare il file di domande. {e}\")\n",
    "        return\n",
    "\n",
    "    # Seleziona solo le domande da processare a partire da START_INDEX\n",
    "    questions_to_process = all_questions_data[START_INDEX:]\n",
    "    \n",
    "    # Lista per salvare i nuovi risultati di questa sessione\n",
    "    qa_results_this_session = []\n",
    "    \n",
    "    print(f\"Inizio generazione risposte per {len(questions_to_process)} domande (partendo dall'indice {START_INDEX})...\")\n",
    "\n",
    "    try:\n",
    "        for i, item in enumerate(tqdm(questions_to_process, desc=\"Generando risposte RAG\"), start=START_INDEX):\n",
    "            question_text = item.get(\"question\")\n",
    "            doc_index = item.get(\"document_index\")\n",
    "\n",
    "            if not question_text or doc_index is None:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                context_document = original_documents[doc_index]\n",
    "                source_url = context_document.metadata.get(\"source_url\", \"N/A\")\n",
    "                context_text = f\"Fonte: {source_url}\\n\\nContenuto:\\n{context_document.text}\"\n",
    "            except IndexError:\n",
    "                print(f\"  -> ATTENZIONE: Indice documento {doc_index} (alla domanda {i}) non trovato. Domanda saltata.\")\n",
    "                continue\n",
    "\n",
    "            answer_text = generate_rag_answer(question_text, context_text, gemini_model)\n",
    "\n",
    "            qa_pair = item.copy()\n",
    "            qa_pair[\"answer\"] = answer_text\n",
    "            qa_results_this_session.append(qa_pair)\n",
    "\n",
    "            sleep_time = random.uniform(MIN_DELAY_SECONDS, MAX_DELAY_SECONDS)\n",
    "            time.sleep(sleep_time)\n",
    "            \n",
    "    finally:\n",
    "        print(\"\\n--- Blocco di salvataggio finale in esecuzione ---\")\n",
    "        if not qa_results_this_session:\n",
    "            print(\"Nessuna nuova risposta da salvare in questa sessione.\")\n",
    "        else:\n",
    "            try:\n",
    "                # Se il file di output esiste già ed è la prima esecuzione (START_INDEX == 0),\n",
    "                # lo sovrascriviamo per iniziare da capo. Altrimenti, aggiungiamo in coda.\n",
    "                mode = 'w' if START_INDEX == 0 else 'a'\n",
    "                \n",
    "                existing_data = []\n",
    "                if mode == 'a' and os.path.exists(OUTPUT_QA_FILE):\n",
    "                    with open(OUTPUT_QA_FILE, 'r', encoding='utf-8') as f:\n",
    "                        try:\n",
    "                            existing_data = json.load(f)\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(\"Attenzione: il file di output esisteva ma era corrotto. Verrà sovrascritto.\")\n",
    "\n",
    "                final_data = existing_data + qa_results_this_session\n",
    "\n",
    "                with open(OUTPUT_QA_FILE, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(final_data, f, ensure_ascii=False, indent=4)\n",
    "                print(f\"Salvataggio completato! Totale di {len(final_data)} coppie Q&A in '{OUTPUT_QA_FILE}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERRORE durante il salvataggio del file JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "HALF = True\n",
    "\n",
    "nome_file_json = 'data/generated_rag_answers.json' \n",
    "\n",
    "# Inizializza le due liste vuote\n",
    "domande = []\n",
    "risposte = []\n",
    "\n",
    "try:\n",
    "    # Apri e leggi il file JSON\n",
    "    with open(nome_file_json, 'r', encoding='utf-8') as f:\n",
    "        dati = json.load(f)\n",
    "\n",
    "    # Scansiona ogni elemento (coppia Q&A) nel file\n",
    "    for elemento in dati:\n",
    "        # Aggiungi la domanda alla lista delle domande\n",
    "        if 'question' in elemento:\n",
    "            if HALF:\n",
    "                if elemento['question_number'] == 1:\n",
    "                    domande.append(elemento['question'])\n",
    "            else:\n",
    "                domande.append(elemento['question'])\n",
    "        \n",
    "        # Aggiungi la risposta alla lista delle risposte\n",
    "        if 'answer' in elemento:\n",
    "            if HALF:\n",
    "                if elemento['question_number'] == 1:\n",
    "                    risposte.append(elemento['answer'])\n",
    "            else:\n",
    "                risposte.append(elemento['answer'])\n",
    "\n",
    "    # Stampa i risultati per verifica\n",
    "    print(\"Processo completato!\")\n",
    "    print(f\"\\n--- LISTA DELLE DOMANDE ({len(domande)}) ---\")\n",
    "    print(domande)\n",
    "\n",
    "    print(f\"\\n--- LISTA DELLE RISPOSTE ({len(risposte)}) ---\")\n",
    "    print(risposte)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRORE: Il file '{nome_file_json}' non è stato trovato.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"ERRORE: Il file '{nome_file_json}' non contiene un JSON valido.\")\n",
    "except Exception as e:\n",
    "    print(f\"Si è verificato un errore inaspettato: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "import random\n",
    "\n",
    "nodes_ssx8 = load_from_pickle(\"nodes/nodes_metadata_sentence_x8.pkl\")\n",
    "nodes_ssx16 = load_from_pickle(\"nodes/nodes_metadata_sentence_x16.pkl\")\n",
    "nodes_hsx16x4x1 = load_from_pickle(\"nodes/nodes_metadata_hierarchical_x16x4x1.pkl\")\n",
    "nodes_hsx8x2x1 = load_from_pickle(\"nodes/nodes_metadata_hierarchical_x8x2x1.pkl\")\n",
    "\n",
    "qa_generate_prompt = (\n",
    "    \"Le informazioni del contesto sono qui sotto.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Data le informazioni del contesto e nessuna conoscenza pregressa, \"\n",
    "    \"genera solo {num_questions_per_chunk} domanda a cui questo contesto può rispondere.\\n\"\n",
    "    \"IMPORTANTE: Il tuo output deve essere solo ed esclusivamente l'elenco delle domande, una per riga. \"\n",
    "    \"NON includere frasi come 'Ecco le domande:', numerazione, o qualsiasi altro testo introduttivo.\\n\"\n",
    ")\n",
    "\n",
    "random.seed(2025)\n",
    "nodes_sample = random.sample(nodes_ssx16, 1000)\n",
    "\n",
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes_sample, llm=eval_llm, num_questions_per_chunk=1, qa_generate_prompt_tmpl=qa_generate_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_responses_sync(questions, query_engine, show_progress = True):\n",
    "    \"\"\"\n",
    "    Una versione sincrona di get_responses. Esegue le query una alla volta.\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    iterator = tqdm(questions) if show_progress else questions\n",
    "    \n",
    "    print(\"Ottenimento delle risposte in modalità sincrona...\")\n",
    "    for question in iterator:\n",
    "        response = query_engine.query(question)\n",
    "        responses.append(response)\n",
    "        \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    BatchEvalRunner,\n",
    ")\n",
    "import nest_asyncio\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\", device=\"cuda\")\n",
    "\n",
    "# define evaluator\n",
    "correctness_evaluator = CorrectnessEvaluator(llm=eval_llm)\n",
    "semanticsimilarity_evaluator = SemanticSimilarityEvaluator(embed_model=embed_model)\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(llm=eval_llm)\n",
    "relevancy_evaluator = RelevancyEvaluator(llm=eval_llm)\n",
    "\n",
    "evaluator_dict = {\n",
    "    \"correctness\": correctness_evaluator,\n",
    "    \"faithfulness\": faithfulness_evaluator,\n",
    "    \"relevancy\": relevancy_evaluator,\n",
    "    \"semantic_similarity\": semanticsimilarity_evaluator,\n",
    "}\n",
    "\n",
    "pred_responses = get_responses_sync(domande, query_engine, show_progress=True)\n",
    "\n",
    "# save_to_pickle(pred_responses, \"results/pred_responses_ssx8_base-retriever.pkl\")\n",
    "\n",
    "batch_runner = BatchEvalRunner(evaluator_dict, workers=2, show_progress=True)\n",
    "\n",
    "eval_results = await batch_runner.aevaluate_responses(\n",
    "    domande, responses=pred_responses, reference=risposte\n",
    ")\n",
    "\n",
    "save_to_pickle(eval_results, \"results/response/eval_results_ssx8_base-retriever.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento oggetti dalla cache 'results/response/eval_results_ssx8_base-retriever.pkl'...\n",
      "Caricati 4 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/response/eval_results_ssx16_base-retriever.pkl'...\n",
      "Caricati 4 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/response/eval_results_ssx8_auto-retriever.pkl'...\n",
      "Caricati 4 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/response/eval_results_ssx16_auto-retriever.pkl'...\n",
      "Caricati 4 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/response/eval_results_hsx16_base-retriever.pkl'...\n",
      "Caricati 4 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/response/eval_results_hsx16_auto-retriever.pkl'...\n",
      "Caricati 4 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/response/eval_results_hsx16_base-retriever_v2.pkl'...\n",
      "Caricati 4 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/response/eval_results_hsx16_auto-retriever_v2.pkl'...\n",
      "Caricati 4 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/response/eval_results_hsx16_auto-retriever_v3.pkl'...\n",
      "Caricati 4 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/response/eval_results_ssx16_base-retriever_v2.pkl'...\n",
      "Caricati 4 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/response/eval_results_hsx8_base-retriever_v3.pkl'...\n",
      "Caricati 4 oggetti.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>correctness</th>\n",
       "      <th>relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>semantic_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SSx16 BaseRetriever 15-15</td>\n",
       "      <td>4.320</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.908555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SSx16 BaseRetriever 10-5</td>\n",
       "      <td>4.250</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.889979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HSx16 AutoRetriever 15-15</td>\n",
       "      <td>4.149</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.884888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SSx8 BaseRetriever 10-5</td>\n",
       "      <td>4.148</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.890420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HSx16 AutoRetriever 10-10</td>\n",
       "      <td>4.009</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.877768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HSx16 AutoRetriever 10-5</td>\n",
       "      <td>3.890</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.872818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SSx16 AutoRetriever 10-5</td>\n",
       "      <td>3.874</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.844344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HSx16 BaseRetriever 10-10</td>\n",
       "      <td>3.821</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.878128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SSx8 AutoRetriever 10-5</td>\n",
       "      <td>3.790</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.836704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HSx16 BaseRetriever 10-5</td>\n",
       "      <td>3.781</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.866446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HSx8 BaseRetriever 15-15</td>\n",
       "      <td>3.497</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.790559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        names  correctness  ...  faithfulness  semantic_similarity\n",
       "0   SSx16 BaseRetriever 15-15        4.320  ...         0.976             0.908555\n",
       "1    SSx16 BaseRetriever 10-5        4.250  ...         0.980             0.889979\n",
       "2   HSx16 AutoRetriever 15-15        4.149  ...         0.982             0.884888\n",
       "3     SSx8 BaseRetriever 10-5        4.148  ...         0.976             0.890420\n",
       "4   HSx16 AutoRetriever 10-10        4.009  ...         0.984             0.877768\n",
       "5    HSx16 AutoRetriever 10-5        3.890  ...         0.976             0.872818\n",
       "6    SSx16 AutoRetriever 10-5        3.874  ...         0.934             0.844344\n",
       "7   HSx16 BaseRetriever 10-10        3.821  ...         0.966             0.878128\n",
       "8     SSx8 AutoRetriever 10-5        3.790  ...         0.930             0.836704\n",
       "9    HSx16 BaseRetriever 10-5        3.781  ...         0.962             0.866446\n",
       "10   HSx8 BaseRetriever 15-15        3.497  ...         0.970             0.790559\n",
       "\n",
       "[11 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.evaluation.eval_utils import get_results_df\n",
    "\n",
    "result = load_from_pickle(\"results/response/eval_results_ssx8_base-retriever.pkl\") # 10-5\n",
    "result2 = load_from_pickle(\"results/response/eval_results_ssx16_base-retriever.pkl\") # 10-5\n",
    "result3 = load_from_pickle(\"results/response/eval_results_ssx8_auto-retriever.pkl\") # 10-5\n",
    "result4 = load_from_pickle(\"results/response/eval_results_ssx16_auto-retriever.pkl\") # 10-5\n",
    "result5 = load_from_pickle(\"results/response/eval_results_hsx16_base-retriever.pkl\") # 10-5\n",
    "result6 = load_from_pickle(\"results/response/eval_results_hsx16_auto-retriever.pkl\") # 10-5\n",
    "result7 = load_from_pickle(\"results/response/eval_results_hsx16_base-retriever_v2.pkl\") # 10-10\n",
    "result8 = load_from_pickle(\"results/response/eval_results_hsx16_auto-retriever_v2.pkl\") # 10-10\n",
    "result9 = load_from_pickle(\"results/response/eval_results_hsx16_auto-retriever_v3.pkl\") # 15-15\n",
    "result10 = load_from_pickle(\"results/response/eval_results_ssx16_base-retriever_v2.pkl\") # 15-15\n",
    "result11 = load_from_pickle(\"results/response/eval_results_hsx8_base-retriever_v3.pkl\") # 15-15\n",
    "\n",
    "results_df = get_results_df(\n",
    "    [result, result2, result3, result4, result5, result6, result7, result8, result9, result10],\n",
    "    [\"SSx8-BaseRetriever\", \"SSx16-BaseRetriever\", \"SSx8-AutoRetriever\", \"SSx16-AutoRetriever\", \"HSx16-BaseRetriever\", \"HSx16-AutoRetriever\", \"HSx16-BaseRetriever-v2\", \"HSx16-AutoRetriever-v2\", \"HSx16-AutoRetriever-v3\", \"SSx16-BaseRetriever-v2\"],\n",
    "    [\"correctness\", \"relevancy\", \"faithfulness\", \"semantic_similarity\"],\n",
    ")\n",
    "\n",
    "results_df2 = get_results_df(\n",
    "    [result10, result2, result9, result, result8, result6, result4, result7, result3, result5, result11],\n",
    "    [\"SSx16 BaseRetriever 15-15\", \"SSx16 BaseRetriever 10-5\", \"HSx16 AutoRetriever 15-15\", \"SSx8 BaseRetriever 10-5\", \"HSx16 AutoRetriever 10-10\", \"HSx16 AutoRetriever 10-5\", \"SSx16 AutoRetriever 10-5\", \"HSx16 BaseRetriever 10-10\", \"SSx8 AutoRetriever 10-5\",  \"HSx16 BaseRetriever 10-5\", \"HSx8 BaseRetriever 15-15\"],\n",
    "    [\"correctness\", \"relevancy\", \"faithfulness\", \"semantic_similarity\"],\n",
    ")\n",
    "display(results_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrival evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\", \"ndcg\"], retriever=base_retriever\n",
    ")\n",
    "\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset, show_progress=True)\n",
    "\n",
    "save_to_pickle(eval_results, \"results/retrieval/eval_results_ssx8_base-retriever.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento oggetti dalla cache 'results/retrieval/retrival_eval_results_ssx16_base_v2.pkl'...\n",
      "Caricati 500 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/retrieval/retrival_eval_results_ssx16_base.pkl'...\n",
      "Caricati 500 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/retrieval/retrival_eval_results_ssx8_base.pkl'...\n",
      "Caricati 500 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/retrieval/retrival_eval_results_hsx16_base.pkl'...\n",
      "Caricati 500 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/retrieval/retrival_eval_results_hsx16_base_v2.pkl'...\n",
      "Caricati 500 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/retrieval/retrival_eval_results_hsx8_base_v2.pkl'...\n",
      "Caricati 500 oggetti.\n",
      "Caricamento oggetti dalla cache 'results/retrieval/retrival_eval_results_hsx8_auto_v2.pkl'...\n",
      "Caricati 500 oggetti.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mrr</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence x16 Base 15-15</th>\n",
       "      <td>0.5285</td>\n",
       "      <td>0.8920</td>\n",
       "      <td>0.6135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence x16 Base 10-5</th>\n",
       "      <td>0.5247</td>\n",
       "      <td>0.8340</td>\n",
       "      <td>0.5992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence x8 Base 10-5</th>\n",
       "      <td>0.4059</td>\n",
       "      <td>0.7160</td>\n",
       "      <td>0.4797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hierarchical x16 Base 15-15</th>\n",
       "      <td>0.0727</td>\n",
       "      <td>0.1640</td>\n",
       "      <td>0.0933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hierarchical x16 Base 10-5</th>\n",
       "      <td>0.0706</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>0.0864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hierarchical x8 Base 15-15</th>\n",
       "      <td>0.0382</td>\n",
       "      <td>0.0960</td>\n",
       "      <td>0.0511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                mrr hit_rate    ndcg\n",
       "Sentence x16 Base 15-15      0.5285   0.8920  0.6135\n",
       "Sentence x16 Base 10-5       0.5247   0.8340  0.5992\n",
       "Sentence x8 Base 10-5        0.4059   0.7160  0.4797\n",
       "Hierarchical x16 Base 15-15  0.0727   0.1640  0.0933\n",
       "Hierarchical x16 Base 10-5   0.0706   0.1380  0.0864\n",
       "Hierarchical x8 Base 15-15   0.0382   0.0960  0.0511"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_average_scores(eval_results_list):\n",
    "    \"\"\"\n",
    "    Funzione di supporto per calcolare i punteggi medi da una \n",
    "    lista di risultati di valutazione del retriever.\n",
    "    Gestisce il formato 'RetrievalEvalResult'.\n",
    "    \"\"\"\n",
    "    scores_per_metric = defaultdict(list)\n",
    "    \n",
    "    if not eval_results_list:\n",
    "        return {}\n",
    "\n",
    "    for query_result in eval_results_list:\n",
    "        if hasattr(query_result, 'metric_dict') and query_result.metric_dict:\n",
    "            for metric_name, metric_value in query_result.metric_dict.items():\n",
    "                \n",
    "                numeric_score = None\n",
    "\n",
    "                if isinstance(metric_value, (int, float)):\n",
    "                    numeric_score = metric_value\n",
    "                elif hasattr(metric_value, 'score') and isinstance(getattr(metric_value, 'score', None), (int, float)):\n",
    "                    numeric_score = metric_value.score\n",
    "                \n",
    "                if numeric_score is not None:\n",
    "                    scores_per_metric[metric_name].append(numeric_score)\n",
    "    \n",
    "    # Calcola le medie\n",
    "    average_scores = {}\n",
    "    for metric_name, scores in scores_per_metric.items():\n",
    "        average_scores[metric_name] = sum(scores) / len(scores) if scores else 0\n",
    "    \n",
    "    return average_scores\n",
    "\n",
    "# --- 1. CARICO I DATI ---\n",
    "result = load_from_pickle(\"results/retrieval/retrival_eval_results_ssx16_base_v2.pkl\") # 15-15\n",
    "result2 = load_from_pickle(\"results/retrieval/retrival_eval_results_ssx16_base.pkl\") # 10-5\n",
    "result3 = load_from_pickle(\"results/retrieval/retrival_eval_results_ssx8_base.pkl\") # 10-5\n",
    "result4 = load_from_pickle(\"results/retrieval/retrival_eval_results_hsx16_base.pkl\") # 10-5\n",
    "result5 = load_from_pickle(\"results/retrieval/retrival_eval_results_hsx16_base_v2.pkl\") # 15-15\n",
    "result6 = load_from_pickle(\"results/retrieval/retrival_eval_results_hsx8_base_v2.pkl\") # 15-15\n",
    "\n",
    "# --- 2. CALCOLO LE MEDIE PER OGNI SET DI RISULTATI ---\n",
    "avg_result = calculate_average_scores(result)\n",
    "avg_result2 = calculate_average_scores(result2)\n",
    "avg_result3 = calculate_average_scores(result3)\n",
    "avg_result4 = calculate_average_scores(result4)\n",
    "avg_result5 = calculate_average_scores(result5)\n",
    "avg_result6 = calculate_average_scores(result6)\n",
    "\n",
    "# --- 3. COMBINO I RISULTATI IN UN DIZIONARIO ---\n",
    "data_for_comparison = {\n",
    "    \"Sentence x16 Base 15-15\": avg_result,\n",
    "    \"Sentence x16 Base 10-5\": avg_result2,\n",
    "    \"Sentence x8 Base 10-5\": avg_result3,\n",
    "    \"Hierarchical x16 Base 15-15\": avg_result5,\n",
    "    \"Hierarchical x16 Base 10-5\": avg_result4,\n",
    "    \"Hierarchical x8 Base 15-15\": avg_result6,\n",
    "}\n",
    "\n",
    "# --- 4. CREO E MOSTRO IL DATAFRAME ---\n",
    "comparison_df = pd.DataFrame(data_for_comparison).T\n",
    "\n",
    "comparison_df_formatted = comparison_df.map(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "display(comparison_df_formatted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
